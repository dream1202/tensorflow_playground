{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-22 16:00:39.703078: epoch 1 avg_loss = 1.141725\n",
      "2017-04-22 16:00:39.723343: epoch 2 avg_loss = 1.150299\n",
      "2017-04-22 16:00:39.754619: epoch 3 avg_loss = 1.189832\n",
      "2017-04-22 16:00:39.779191: epoch 4 avg_loss = 1.165445\n",
      "2017-04-22 16:00:39.801631: epoch 5 avg_loss = 1.097372\n",
      "2017-04-22 16:00:39.826005: epoch 6 avg_loss = 1.159822\n",
      "2017-04-22 16:00:39.849285: epoch 7 avg_loss = 1.134555\n",
      "2017-04-22 16:00:39.871044: epoch 8 avg_loss = 1.118340\n",
      "2017-04-22 16:00:39.895659: epoch 9 avg_loss = 1.127475\n",
      "2017-04-22 16:00:39.926874: epoch 10 avg_loss = 1.162262\n",
      "2017-04-22 16:00:39.958194: epoch 11 avg_loss = 1.140429\n",
      "2017-04-22 16:00:39.980689: epoch 12 avg_loss = 1.078822\n",
      "2017-04-22 16:00:40.003267: epoch 13 avg_loss = 1.131852\n",
      "2017-04-22 16:00:40.025395: epoch 14 avg_loss = 1.110271\n",
      "2017-04-22 16:00:40.043379: epoch 15 avg_loss = 1.093308\n",
      "2017-04-22 16:00:40.064978: epoch 16 avg_loss = 1.104280\n",
      "2017-04-22 16:00:40.089103: epoch 17 avg_loss = 1.130807\n",
      "2017-04-22 16:00:40.110499: epoch 18 avg_loss = 1.112330\n",
      "2017-04-22 16:00:40.132902: epoch 19 avg_loss = 1.055256\n",
      "2017-04-22 16:00:40.160200: epoch 20 avg_loss = 1.099004\n",
      "2017-04-22 16:00:40.185373: epoch 21 avg_loss = 1.081318\n",
      "2017-04-22 16:00:40.208436: epoch 22 avg_loss = 1.061286\n",
      "2017-04-22 16:00:40.228732: epoch 23 avg_loss = 1.075222\n",
      "2017-04-22 16:00:40.252277: epoch 24 avg_loss = 1.091240\n",
      "2017-04-22 16:00:40.277499: epoch 25 avg_loss = 1.083483\n",
      "2017-04-22 16:00:40.300650: epoch 26 avg_loss = 1.024084\n",
      "2017-04-22 16:00:40.324447: epoch 27 avg_loss = 1.059226\n",
      "2017-04-22 16:00:40.347593: epoch 28 avg_loss = 1.047863\n",
      "2017-04-22 16:00:40.372630: epoch 29 avg_loss = 1.028274\n",
      "2017-04-22 16:00:40.394254: epoch 30 avg_loss = 1.045746\n",
      "2017-04-22 16:00:40.418483: epoch 31 avg_loss = 1.055929\n",
      "2017-04-22 16:00:40.439121: epoch 32 avg_loss = 1.058996\n",
      "2017-04-22 16:00:40.460744: epoch 33 avg_loss = 1.000172\n",
      "2017-04-22 16:00:40.485865: epoch 34 avg_loss = 1.027723\n",
      "2017-04-22 16:00:40.509872: epoch 35 avg_loss = 1.023432\n",
      "2017-04-22 16:00:40.531844: epoch 36 avg_loss = 1.011050\n",
      "2017-04-22 16:00:40.551858: epoch 37 avg_loss = 1.024911\n",
      "2017-04-22 16:00:40.578792: epoch 38 avg_loss = 1.029594\n",
      "2017-04-22 16:00:40.601592: epoch 39 avg_loss = 1.037030\n",
      "2017-04-22 16:00:40.624817: epoch 40 avg_loss = 0.981397\n",
      "2017-04-22 16:00:40.649166: epoch 41 avg_loss = 1.000427\n",
      "2017-04-22 16:00:40.669304: epoch 42 avg_loss = 1.001483\n",
      "2017-04-22 16:00:40.691117: epoch 43 avg_loss = 1.003226\n",
      "2017-04-22 16:00:40.715679: epoch 44 avg_loss = 1.005328\n",
      "2017-04-22 16:00:40.740098: epoch 45 avg_loss = 1.006302\n",
      "2017-04-22 16:00:40.763177: epoch 46 avg_loss = 1.017489\n",
      "2017-04-22 16:00:40.789598: epoch 47 avg_loss = 0.966432\n",
      "2017-04-22 16:00:40.812689: epoch 48 avg_loss = 0.979963\n",
      "2017-04-22 16:00:40.835203: epoch 49 avg_loss = 0.984017\n",
      "2017-04-22 16:00:40.856446: epoch 50 avg_loss = 0.995958\n",
      "2017-04-22 16:00:40.876806: epoch 51 avg_loss = 0.989450\n",
      "2017-04-22 16:00:40.899855: epoch 52 avg_loss = 0.990213\n",
      "2017-04-22 16:00:40.921880: epoch 53 avg_loss = 1.001953\n",
      "2017-04-22 16:00:40.943795: epoch 54 avg_loss = 0.953090\n",
      "2017-04-22 16:00:40.966676: epoch 55 avg_loss = 0.966144\n",
      "2017-04-22 16:00:40.994678: epoch 56 avg_loss = 0.966665\n",
      "2017-04-22 16:00:41.016518: epoch 57 avg_loss = 0.982597\n",
      "2017-04-22 16:00:41.032235: epoch 58 avg_loss = 0.972210\n",
      "2017-04-22 16:00:41.053075: epoch 59 avg_loss = 0.976491\n",
      "2017-04-22 16:00:41.069940: epoch 60 avg_loss = 0.984478\n",
      "2017-04-22 16:00:41.091361: epoch 61 avg_loss = 0.936245\n",
      "2017-04-22 16:00:41.115200: epoch 62 avg_loss = 0.952388\n",
      "2017-04-22 16:00:41.138546: epoch 63 avg_loss = 0.947213\n",
      "2017-04-22 16:00:41.160691: epoch 64 avg_loss = 0.965541\n",
      "2017-04-22 16:00:41.181582: epoch 65 avg_loss = 0.954541\n",
      "2017-04-22 16:00:41.209801: epoch 66 avg_loss = 0.963660\n",
      "2017-04-22 16:00:41.231131: epoch 67 avg_loss = 0.967648\n",
      "2017-04-22 16:00:41.253521: epoch 68 avg_loss = 0.920560\n",
      "2017-04-22 16:00:41.278465: epoch 69 avg_loss = 0.939724\n",
      "2017-04-22 16:00:41.302832: epoch 70 avg_loss = 0.930777\n",
      "2017-04-22 16:00:41.325282: epoch 71 avg_loss = 0.951177\n",
      "2017-04-22 16:00:41.345003: epoch 72 avg_loss = 0.939922\n",
      "2017-04-22 16:00:41.368507: epoch 73 avg_loss = 0.951039\n",
      "2017-04-22 16:00:41.391189: epoch 74 avg_loss = 0.953604\n",
      "2017-04-22 16:00:41.417302: epoch 75 avg_loss = 0.907436\n",
      "2017-04-22 16:00:41.443011: epoch 76 avg_loss = 0.928411\n",
      "2017-04-22 16:00:41.466892: epoch 77 avg_loss = 0.916445\n",
      "2017-04-22 16:00:41.488640: epoch 78 avg_loss = 0.938847\n",
      "2017-04-22 16:00:41.508153: epoch 79 avg_loss = 0.926687\n",
      "2017-04-22 16:00:41.532368: epoch 80 avg_loss = 0.938570\n",
      "2017-04-22 16:00:41.554180: epoch 81 avg_loss = 0.940839\n",
      "2017-04-22 16:00:41.576598: epoch 82 avg_loss = 0.895208\n",
      "2017-04-22 16:00:41.600806: epoch 83 avg_loss = 0.916752\n",
      "2017-04-22 16:00:41.627897: epoch 84 avg_loss = 0.902890\n",
      "2017-04-22 16:00:41.650426: epoch 85 avg_loss = 0.926860\n",
      "2017-04-22 16:00:41.670302: epoch 86 avg_loss = 0.913546\n",
      "2017-04-22 16:00:41.695630: epoch 87 avg_loss = 0.926423\n",
      "2017-04-22 16:00:41.721877: epoch 88 avg_loss = 0.928196\n",
      "2017-04-22 16:00:41.747639: epoch 89 avg_loss = 0.882654\n",
      "2017-04-22 16:00:41.771043: epoch 90 avg_loss = 0.904150\n",
      "2017-04-22 16:00:41.796004: epoch 91 avg_loss = 0.889306\n",
      "2017-04-22 16:00:41.820365: epoch 92 avg_loss = 0.914635\n",
      "2017-04-22 16:00:41.846529: epoch 93 avg_loss = 0.900101\n",
      "2017-04-22 16:00:41.871548: epoch 94 avg_loss = 0.913765\n",
      "2017-04-22 16:00:41.894319: epoch 95 avg_loss = 0.915329\n",
      "2017-04-22 16:00:41.918153: epoch 96 avg_loss = 0.869582\n",
      "2017-04-22 16:00:41.943015: epoch 97 avg_loss = 0.891008\n",
      "2017-04-22 16:00:41.966291: epoch 98 avg_loss = 0.875368\n",
      "2017-04-22 16:00:41.988647: epoch 99 avg_loss = 0.902079\n",
      "2017-04-22 16:00:42.009340: epoch 100 avg_loss = 0.886254\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Sequence Labeling with variable-length sequences\n",
    "####\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "NUM_HIDDEN = 5\n",
    "NUM_CLASS = 3\n",
    "FEATURE_SIZE_PER_TIMESTEP = 5\n",
    "\n",
    "### Data pipeline\n",
    "def input_pipeline(filename, batch_size, epochs=None):\n",
    "    file_list = [os.path.join(os.getcwd(), filename)]\n",
    "    file_queue = tf.train.string_input_producer(file_list, num_epochs=epochs)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(file_queue)\n",
    "    sequence_features = {\n",
    "        \"inputs\": tf.FixedLenSequenceFeature([FEATURE_SIZE_PER_TIMESTEP], dtype=tf.float32),\n",
    "        \"labels\": tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "    }\n",
    "    _, sequence = tf.parse_single_sequence_example(\n",
    "        serialized=serialized_example,\n",
    "        sequence_features=sequence_features)\n",
    "\n",
    "    actual_length = tf.shape(sequence[\"inputs\"])[0]\n",
    "    batch_lengths, batch_sequences, batch_labels = tf.train.batch(\n",
    "        [actual_length, sequence[\"inputs\"], sequence[\"labels\"]],\n",
    "        batch_size=batch_size,\n",
    "        dynamic_pad=True,\n",
    "        allow_smaller_final_batch=True,\n",
    "        name=\"input_batching\")\n",
    "    return batch_lengths, batch_sequences, batch_labels\n",
    "\n",
    "\n",
    "### Build Model\n",
    "def inference(inputs, actual_lengths):\n",
    "    cell = tf.contrib.rnn.LSTMCell(NUM_HIDDEN)\n",
    "    outputs, current_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, sequence_length=actual_lengths)\n",
    "    max_length = tf.shape(outputs)[1]\n",
    "    # Output layer weights & biases\n",
    "    weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_CLASS]), dtype=tf.float32)\n",
    "    biases = tf.Variable(tf.constant(0.1, shape=[NUM_CLASS]), dtype=tf.float32)\n",
    "    # Flatten to apply same weights to all time steps.\n",
    "    outputs_flat = tf.reshape(outputs, [-1, NUM_HIDDEN])\n",
    "    logits_flat = tf.add(tf.matmul(outputs_flat, weights), biases)\n",
    "    predictions_flat = tf.nn.softmax(logits_flat)\n",
    "    logits = tf.reshape(logits_flat, [-1, max_length, NUM_CLASS])\n",
    "    predictions = tf.reshape(predictions_flat, [-1, max_length, NUM_CLASS])\n",
    "    return logits, predictions\n",
    "\n",
    "\n",
    "## Cost function\n",
    "def loss(logits, labels, actual_lengths):\n",
    "    logits_flat = tf.reshape(logits, [-1, NUM_CLASS])\n",
    "    labels_flat = tf.reshape(labels, [-1])\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_flat, labels=labels_flat)\n",
    "    # Mask the losses via sequence length\n",
    "    max_length = tf.shape(labels)[1]\n",
    "    mask = tf.sequence_mask(actual_lengths, max_length, dtype=tf.float32)\n",
    "    mask = tf.reshape(mask, [-1])\n",
    "    masked_losses = tf.multiply(mask, losses)\n",
    "    masked_losses = tf.reshape(masked_losses, tf.shape(labels))\n",
    "    # Calculate mean loss\n",
    "    mean_loss_by_example = tf.reduce_sum(masked_losses, axis=1) / tf.cast(actual_lengths, tf.float32)\n",
    "    mean_loss = tf.reduce_mean(mean_loss_by_example)\n",
    "    return mean_loss\n",
    "\n",
    "    \n",
    "## Error tracking \n",
    "def error(predictions, labels, actual_lengths):\n",
    "    predictions_flat = tf.reshape(predictions, [-1, NUM_CLASS])\n",
    "    labels_flat = tf.reshape(labels, [-1])\n",
    "    errors = tf.not_equal(labels_flat, tf.argmax(predictions_flat, 1))\n",
    "    errors = tf.cast(errors, tf.float32)\n",
    "    max_length = tf.shape(labels)[1]\n",
    "    mask = tf.sequence_mask(acutal_lengths, max_length, dtype=tf.float32)\n",
    "    mask = tf.reshape(mask, [-1])\n",
    "    masked_errors = mask * errors\n",
    "    masked_errors = tf.reshape(masked_errors, tf.shape(labels))\n",
    "    mean_error_by_example = tf.reduce_sum(masked_errors, axis=1) / actual_lengths\n",
    "    mean_error = tf.reduce_mean(mean_error_by_example)\n",
    "    return mean_error\n",
    "    \n",
    "# # Calculate the losses ver 2\n",
    "# cross_entropy = tf.one_hot(y_flat, NUM_CLASS) * tf.log(probs_flat)\n",
    "# cross_entropy = -tf.reduce_sum(cross_entropy, axis=1)\n",
    "# masked_ce = mask * cross_entropy\n",
    "# masked_ce = tf.reshape(masked_ce, tf.shape(y))\n",
    "# mean_ce_by_example = tf.reduce_sum(masked_losses, axis=1) / example_len\n",
    "# mean_ce = tf.reduce_mean(mean_ce_by_example)\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)    \n",
    "    return train_op\n",
    "\n",
    "### Training\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 3\n",
    "DISPLAY_STEP = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAINING_SET_SIZE = 7\n",
    "\n",
    "filename = 'Sequence_test_3.tfr'\n",
    "with tf.Graph().as_default():\n",
    "    tf.set_random_seed(10)\n",
    "    np.random.seed(10)    \n",
    "    # Build Graph\n",
    "    lengths, sequences, labels = input_pipeline(filename, BATCH_SIZE)\n",
    "    logits, _ = inference(sequences, lengths)\n",
    "    avg_loss = loss(logits, labels, lengths)\n",
    "    train_op = training(avg_loss, LEARNING_RATE)\n",
    "    \n",
    "    # Create & Initialize Session\n",
    "    sess = tf.Session()\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # Start QueueRunner\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    try: \n",
    "        # Training cycles\n",
    "        for epoch in range(1, NUM_EPOCHS+1):\n",
    "            epoch_avg_loss = 0.0\n",
    "            total_batch = int(TRAINING_SET_SIZE / BATCH_SIZE)\n",
    "            for step in range(1, total_batch +1):\n",
    "                if coord.should_stop():\n",
    "                    break\n",
    "                _, train_loss = sess.run([train_op, avg_loss]) \n",
    "                epoch_avg_loss += train_loss / total_batch\n",
    "                assert not np.isnan(train_loss), 'Model diverged with loss = NaN'\n",
    "                \n",
    "                if step % DISPLAY_STEP == 0:\n",
    "                    print('%s: epoch %d, step %d, train_loss = %.6f'\n",
    "                        % (datetime.now(), epoch, step, train_loss))\n",
    "                \n",
    "            print('%s: epoch %d avg_loss = %.6f'\n",
    "                % (datetime.now(), epoch, epoch_avg_loss))\n",
    "                \n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        print(e.error_code, e.message)\n",
    "        print('Done!')\n",
    "    \n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "    \n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "    \n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.66392612  2.02395773  2.23962045  1.61670065  0.14862178  0.19047575\n",
      "  1.26339984  1.89619958  1.09861231]\n",
      "[ 0.66392612  2.02395773  2.23962045  1.61670065  0.14862178  0.19047575\n",
      "  1.26339984  1.89619958  0.        ]\n",
      "[[ 0.66392612  2.02395773  2.23962045]\n",
      " [ 1.61670065  0.14862178  0.19047575]\n",
      " [ 1.26339984  1.89619958  0.        ]]\n",
      "[ 1.64250147  0.65193272  1.57979965]\n",
      "1.291411280632019\n"
     ]
    }
   ],
   "source": [
    "## Illustration of intermediate results, run-time values excerpted from tfdbg\n",
    "tf.reset_default_graph()\n",
    "labels_flat = np.array([0,1,2,1,0,0,1,1,0])\n",
    "logits_flat = np.array([[0.6422022 , -0.08686169, -0.13423285],\n",
    "                        [0.88595915, -0.87825918, -1.19131315],\n",
    "                        [1.09363282, -0.92535424, -0.90869629],\n",
    "                        [1.34931934,  0.12072743, -0.35725975],\n",
    "                        [2.14791036, -0.19916281, -0.59185725],\n",
    "                        [1.07656074, -1.0128665 , -1.37606812],\n",
    "                        [0.41977212, -0.05744966, -0.13450697],\n",
    "                        [1.32846284, -0.24778995, -0.44169524],\n",
    "                        [0.1       ,  0.1       ,  0.1       ]], dtype=np.float32)\n",
    "losses_flat = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_flat, labels=labels_flat)\n",
    "\n",
    "actual_lengths = np.array([3,3,2])\n",
    "seq_mask_flat = np.array([1.,1.,1.,1.,1.,1.,1.,1.,0.], dtype=np.float32)\n",
    "masked_losses_flat = tf.multiply(seq_mask_flat, losses_flat)\n",
    "labels = tf.reshape(labels_flat, [3, -1])\n",
    "masked_losses = tf.reshape(masked_losses_flat, tf.shape(labels))\n",
    "mean_loss_by_example = tf.div(tf.reduce_sum(masked_losses, axis=1), tf.cast(actual_lengths, tf.float32))\n",
    "mean_loss = tf.reduce_mean(mean_loss_by_example)                            \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    a, b, c, d, e = sess.run([losses_flat, masked_losses_flat, masked_losses, mean_loss_by_example, mean_loss])\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "    print(d)\n",
    "    print(\"%.15f\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.0991137   1.43568242  2.01699328  0.20474197  1.0992794   3.00335884\n",
      "  0.09546375  1.0992794   1.0992794   1.0992794   0.19948083  2.47502112\n",
      "  2.28775692  2.53907585  0.22659162]\n",
      "[ 1.0991137   1.43568242  2.01699328  0.20474197  0.          3.00335884\n",
      "  0.09546375  0.          0.          0.          0.19948083  2.47502112\n",
      "  2.28775692  2.53907585  0.22659162]\n",
      "[[ 1.0991137   1.43568242  2.01699328  0.20474197  0.        ]\n",
      " [ 3.00335884  0.09546375  0.          0.          0.        ]\n",
      " [ 0.19948083  2.47502112  2.28775692  2.53907585  0.22659162]]\n",
      "[ 1.18913281  1.5494113   1.54558539]\n",
      "1.428043007850647\n"
     ]
    }
   ],
   "source": [
    "## Illustration of intermediate results, run-time values excerpted from tfdbg\n",
    "\n",
    "tf.reset_default_graph()\n",
    "labels_flat = np.array([2, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 2, 0])\n",
    "logits_flat = np.array([[0.09829693 , 0.1014218 ,  0.09910872],\n",
    "                        [0.49634132, -0.2791599 , -0.24879429],\n",
    "                        [0.57596284, -1.16146052, -1.34416914],\n",
    "                        [1.42855453, -0.80200392, -0.69387823],\n",
    "                        [0.099     ,  0.101     ,  0.09900001],\n",
    "                        [2.36766672, -0.05113668, -0.49951288],\n",
    "                        [1.89118695, -1.02813923, -1.18361068],\n",
    "                        [0.099     ,  0.101     ,  0.09900001],\n",
    "                        [0.099     ,  0.101     ,  0.09900001],\n",
    "                        [0.099     ,  0.101     ,  0.09900001],\n",
    "                        [1.59493768, -0.56841475, -0.65098649],\n",
    "                        [1.08355463, -1.22900999, -1.47534275],\n",
    "                        [1.36640537, -0.7257424 , -1.01273966],\n",
    "                        [1.60844588, -0.58185029, -0.74234509],\n",
    "                        [1.38049722, -0.61191964, -0.75697964],\n",
    "                       ], dtype=np.float32)\n",
    "losses_flat = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_flat, labels=labels_flat)\n",
    "\n",
    "actual_lengths = np.array([4,2,5])\n",
    "seq_mask_flat = np.array([1.,1.,1.,1.,0.,1.,1.,0.,0.,0.,1.,1.,1.,1.,1.], dtype=np.float32)\n",
    "masked_losses_flat = tf.multiply(seq_mask_flat, losses_flat)\n",
    "labels = tf.reshape(labels_flat, [3, -1])\n",
    "masked_losses = tf.reshape(masked_losses_flat, tf.shape(labels))\n",
    "mean_loss_by_example = tf.div(tf.reduce_sum(masked_losses, axis=1), tf.cast(actual_lengths, tf.float32))\n",
    "mean_loss = tf.reduce_mean(mean_loss_by_example)                            \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    a, b, c, d, e = sess.run([losses_flat, masked_losses_flat, masked_losses, mean_loss_by_example, mean_loss])\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "    print(d)\n",
    "    print(\"%.15f\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# References:\n",
    "# (1) http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\n",
    "# (2) https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/\n",
    "# (3) https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "# (4) https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py)\n",
    "# (5) https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/dynamic_rnn.py\n",
    "# (6) https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
