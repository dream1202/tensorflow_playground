{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-24 14:39:05.919187: epoch 1 avg_loss = 1.147794\n",
      "2017-04-24 14:39:05.957375: epoch 2 avg_loss = 1.173444\n",
      "2017-04-24 14:39:05.992926: epoch 3 avg_loss = 1.125526\n",
      "2017-04-24 14:39:06.025304: epoch 4 avg_loss = 1.156233\n",
      "2017-04-24 14:39:06.057438: epoch 5 avg_loss = 1.155858\n",
      "2017-04-24 14:39:06.089595: epoch 6 avg_loss = 1.097722\n",
      "2017-04-24 14:39:06.128443: epoch 7 avg_loss = 1.140637\n",
      "2017-04-24 14:39:06.165273: epoch 8 avg_loss = 1.113705\n",
      "2017-04-24 14:39:06.201642: epoch 9 avg_loss = 1.132878\n",
      "2017-04-24 14:39:06.232779: epoch 10 avg_loss = 1.090742\n",
      "2017-04-24 14:39:06.262576: epoch 11 avg_loss = 1.116564\n",
      "2017-04-24 14:39:06.294061: epoch 12 avg_loss = 1.115048\n",
      "2017-04-24 14:39:06.325620: epoch 13 avg_loss = 1.062377\n",
      "2017-04-24 14:39:06.363779: epoch 14 avg_loss = 1.094674\n",
      "2017-04-24 14:39:06.393786: epoch 15 avg_loss = 1.072043\n",
      "2017-04-24 14:39:06.428580: epoch 16 avg_loss = 1.079789\n",
      "2017-04-24 14:39:06.462332: epoch 17 avg_loss = 1.044403\n",
      "2017-04-24 14:39:06.494654: epoch 18 avg_loss = 1.066792\n",
      "2017-04-24 14:39:06.526553: epoch 19 avg_loss = 1.067295\n",
      "2017-04-24 14:39:06.557802: epoch 20 avg_loss = 1.013961\n",
      "2017-04-24 14:39:06.598562: epoch 21 avg_loss = 1.044678\n",
      "2017-04-24 14:39:06.627533: epoch 22 avg_loss = 1.032054\n",
      "2017-04-24 14:39:06.659179: epoch 23 avg_loss = 1.031606\n",
      "2017-04-24 14:39:06.694073: epoch 24 avg_loss = 1.009864\n",
      "2017-04-24 14:39:06.725387: epoch 25 avg_loss = 1.025082\n",
      "2017-04-24 14:39:06.758135: epoch 26 avg_loss = 1.035941\n",
      "2017-04-24 14:39:06.790872: epoch 27 avg_loss = 0.986316\n",
      "2017-04-24 14:39:06.831442: epoch 28 avg_loss = 1.002555\n",
      "2017-04-24 14:39:06.863771: epoch 29 avg_loss = 1.008728\n",
      "2017-04-24 14:39:06.899398: epoch 30 avg_loss = 1.001176\n",
      "2017-04-24 14:39:06.934714: epoch 31 avg_loss = 0.986312\n",
      "2017-04-24 14:39:06.968782: epoch 32 avg_loss = 0.989610\n",
      "2017-04-24 14:39:07.000992: epoch 33 avg_loss = 1.013305\n",
      "2017-04-24 14:39:07.032626: epoch 34 avg_loss = 0.966312\n",
      "2017-04-24 14:39:07.075384: epoch 35 avg_loss = 0.972778\n",
      "2017-04-24 14:39:07.107009: epoch 36 avg_loss = 0.990726\n",
      "2017-04-24 14:39:07.141783: epoch 37 avg_loss = 0.979073\n",
      "2017-04-24 14:39:07.178145: epoch 38 avg_loss = 0.964531\n",
      "2017-04-24 14:39:07.212037: epoch 39 avg_loss = 0.967366\n",
      "2017-04-24 14:39:07.256880: epoch 40 avg_loss = 0.988086\n",
      "2017-04-24 14:39:07.297493: epoch 41 avg_loss = 0.941100\n",
      "2017-04-24 14:39:07.342130: epoch 42 avg_loss = 0.949464\n",
      "2017-04-24 14:39:07.372727: epoch 43 avg_loss = 0.966237\n",
      "2017-04-24 14:39:07.406940: epoch 44 avg_loss = 0.956258\n",
      "2017-04-24 14:39:07.441421: epoch 45 avg_loss = 0.938660\n",
      "2017-04-24 14:39:07.474683: epoch 46 avg_loss = 0.946627\n",
      "2017-04-24 14:39:07.508983: epoch 47 avg_loss = 0.963193\n",
      "2017-04-24 14:39:07.545237: epoch 48 avg_loss = 0.918058\n",
      "2017-04-24 14:39:07.583030: epoch 49 avg_loss = 0.929428\n",
      "2017-04-24 14:39:07.614835: epoch 50 avg_loss = 0.945291\n",
      "2017-04-24 14:39:07.656402: epoch 51 avg_loss = 0.936594\n",
      "2017-04-24 14:39:07.699189: epoch 52 avg_loss = 0.919207\n",
      "2017-04-24 14:39:07.735001: epoch 53 avg_loss = 0.928248\n",
      "2017-04-24 14:39:07.776709: epoch 54 avg_loss = 0.942483\n",
      "2017-04-24 14:39:07.805319: epoch 55 avg_loss = 0.899291\n",
      "2017-04-24 14:39:07.841267: epoch 56 avg_loss = 0.910609\n",
      "2017-04-24 14:39:07.878801: epoch 57 avg_loss = 0.926752\n",
      "2017-04-24 14:39:07.912850: epoch 58 avg_loss = 0.917801\n",
      "2017-04-24 14:39:07.952795: epoch 59 avg_loss = 0.900499\n",
      "2017-04-24 14:39:07.987777: epoch 60 avg_loss = 0.909501\n",
      "2017-04-24 14:39:08.019848: epoch 61 avg_loss = 0.923073\n",
      "2017-04-24 14:39:08.056443: epoch 62 avg_loss = 0.879622\n",
      "2017-04-24 14:39:08.085563: epoch 63 avg_loss = 0.891185\n",
      "2017-04-24 14:39:08.121940: epoch 64 avg_loss = 0.907932\n",
      "2017-04-24 14:39:08.166812: epoch 65 avg_loss = 0.897926\n",
      "2017-04-24 14:39:08.195796: epoch 66 avg_loss = 0.881044\n",
      "2017-04-24 14:39:08.230227: epoch 67 avg_loss = 0.889506\n",
      "2017-04-24 14:39:08.265514: epoch 68 avg_loss = 0.902921\n",
      "2017-04-24 14:39:08.296421: epoch 69 avg_loss = 0.859125\n",
      "2017-04-24 14:39:08.325879: epoch 70 avg_loss = 0.870432\n",
      "2017-04-24 14:39:08.351720: epoch 71 avg_loss = 0.887888\n",
      "2017-04-24 14:39:08.369328: epoch 72 avg_loss = 0.877048\n",
      "2017-04-24 14:39:08.413967: epoch 73 avg_loss = 0.860783\n",
      "2017-04-24 14:39:08.444402: epoch 74 avg_loss = 0.868057\n",
      "2017-04-24 14:39:08.474026: epoch 75 avg_loss = 0.882069\n",
      "2017-04-24 14:39:08.503933: epoch 76 avg_loss = 0.838116\n",
      "2017-04-24 14:39:08.536470: epoch 77 avg_loss = 0.848450\n",
      "2017-04-24 14:39:08.575064: epoch 78 avg_loss = 0.867188\n",
      "2017-04-24 14:39:08.611359: epoch 79 avg_loss = 0.855930\n",
      "2017-04-24 14:39:08.639912: epoch 80 avg_loss = 0.840224\n",
      "2017-04-24 14:39:08.674726: epoch 81 avg_loss = 0.845609\n",
      "2017-04-24 14:39:08.713304: epoch 82 avg_loss = 0.861692\n",
      "2017-04-24 14:39:08.750448: epoch 83 avg_loss = 0.817556\n",
      "2017-04-24 14:39:08.789213: epoch 84 avg_loss = 0.825510\n",
      "2017-04-24 14:39:08.817887: epoch 85 avg_loss = 0.847009\n",
      "2017-04-24 14:39:08.851000: epoch 86 avg_loss = 0.835412\n",
      "2017-04-24 14:39:08.882631: epoch 87 avg_loss = 0.819574\n",
      "2017-04-24 14:39:08.913000: epoch 88 avg_loss = 0.822507\n",
      "2017-04-24 14:39:08.944946: epoch 89 avg_loss = 0.842278\n",
      "2017-04-24 14:39:08.979895: epoch 90 avg_loss = 0.797441\n",
      "2017-04-24 14:39:09.017809: epoch 91 avg_loss = 0.802089\n",
      "2017-04-24 14:39:09.056153: epoch 92 avg_loss = 0.827389\n",
      "2017-04-24 14:39:09.091795: epoch 93 avg_loss = 0.815705\n",
      "2017-04-24 14:39:09.126085: epoch 94 avg_loss = 0.799234\n",
      "2017-04-24 14:39:09.156011: epoch 95 avg_loss = 0.799983\n",
      "2017-04-24 14:39:09.185252: epoch 96 avg_loss = 0.823621\n",
      "2017-04-24 14:39:09.215623: epoch 97 avg_loss = 0.777937\n",
      "2017-04-24 14:39:09.247801: epoch 98 avg_loss = 0.779991\n",
      "2017-04-24 14:39:09.276886: epoch 99 avg_loss = 0.808097\n",
      "2017-04-24 14:39:09.313171: epoch 100 avg_loss = 0.796986\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Sequence Labeling with variable-length sequences\n",
    "####\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "NUM_HIDDEN = 5\n",
    "NUM_CLASS = 3\n",
    "FEATURE_SIZE_PER_TIMESTEP = 5\n",
    "\n",
    "### Data pipeline\n",
    "def input_pipeline(filename, batch_size, epochs=None):\n",
    "    file_list = [os.path.join(os.getcwd(), filename)]\n",
    "    file_queue = tf.train.string_input_producer(file_list, num_epochs=epochs)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(file_queue)\n",
    "    sequence_features = {\n",
    "        \"inputs\": tf.FixedLenSequenceFeature([FEATURE_SIZE_PER_TIMESTEP], dtype=tf.float32),\n",
    "        \"labels\": tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "    }\n",
    "    _, sequence = tf.parse_single_sequence_example(\n",
    "        serialized=serialized_example,\n",
    "        sequence_features=sequence_features)\n",
    "\n",
    "    actual_length = tf.shape(sequence[\"inputs\"])[0]\n",
    "    batch_lengths, batch_sequences, batch_labels = tf.train.batch(\n",
    "        [actual_length, sequence[\"inputs\"], sequence[\"labels\"]],\n",
    "        batch_size=batch_size,\n",
    "        dynamic_pad=True,\n",
    "        allow_smaller_final_batch=True,\n",
    "        name=\"input_batching\")\n",
    "    return batch_lengths, batch_sequences, batch_labels\n",
    "\n",
    "\n",
    "### Build Model\n",
    "def inference(inputs, actual_lengths):\n",
    "    cell = tf.contrib.rnn.LSTMCell(NUM_HIDDEN)\n",
    "    outputs, current_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, sequence_length=actual_lengths)\n",
    "    max_length = tf.shape(outputs)[1]\n",
    "    # Output layer weights & biases\n",
    "    weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_CLASS]), dtype=tf.float32)\n",
    "    biases = tf.Variable(tf.constant(0.1, shape=[NUM_CLASS]), dtype=tf.float32)\n",
    "    # Flatten to apply same weights to all time steps.\n",
    "    outputs_flat = tf.reshape(outputs, [-1, NUM_HIDDEN])\n",
    "    logits_flat = tf.add(tf.matmul(outputs_flat, weights), biases)\n",
    "    predictions_flat = tf.nn.softmax(logits_flat)\n",
    "    logits = tf.reshape(logits_flat, [-1, max_length, NUM_CLASS])\n",
    "    predictions = tf.reshape(predictions_flat, [-1, max_length, NUM_CLASS])\n",
    "    return logits, predictions\n",
    "\n",
    "\n",
    "## Cost function\n",
    "def loss(logits, labels, actual_lengths):\n",
    "    logits_flat = tf.reshape(logits, [-1, NUM_CLASS])\n",
    "    labels_flat = tf.reshape(labels, [-1])\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_flat, labels=labels_flat)\n",
    "    # Mask the losses via sequence length\n",
    "    max_length = tf.shape(labels)[1]\n",
    "    mask = tf.sequence_mask(actual_lengths, max_length, dtype=tf.float32)\n",
    "    mask = tf.reshape(mask, [-1])\n",
    "    masked_losses = tf.multiply(mask, losses)\n",
    "    masked_losses = tf.reshape(masked_losses, tf.shape(labels))\n",
    "    # Calculate mean loss\n",
    "    mean_loss_by_example = tf.reduce_sum(masked_losses, axis=1) / tf.cast(actual_lengths, tf.float32)\n",
    "    mean_loss = tf.reduce_mean(mean_loss_by_example)\n",
    "    return mean_loss\n",
    "\n",
    "    \n",
    "## Error tracking \n",
    "def error(predictions, labels, actual_lengths):\n",
    "    predictions_flat = tf.reshape(predictions, [-1, NUM_CLASS])\n",
    "    labels_flat = tf.reshape(labels, [-1])\n",
    "    errors = tf.not_equal(labels_flat, tf.argmax(predictions_flat, 1))\n",
    "    errors = tf.cast(errors, tf.float32)\n",
    "    max_length = tf.shape(labels)[1]\n",
    "    mask = tf.sequence_mask(acutal_lengths, max_length, dtype=tf.float32)\n",
    "    mask = tf.reshape(mask, [-1])\n",
    "    masked_errors = mask * errors\n",
    "    masked_errors = tf.reshape(masked_errors, tf.shape(labels))\n",
    "    mean_error_by_example = tf.reduce_sum(masked_errors, axis=1) / actual_lengths\n",
    "    mean_error = tf.reduce_mean(mean_error_by_example)\n",
    "    return mean_error\n",
    "    \n",
    "# # Calculate the losses ver 2\n",
    "# cross_entropy = tf.one_hot(y_flat, NUM_CLASS) * tf.log(probs_flat)\n",
    "# cross_entropy = -tf.reduce_sum(cross_entropy, axis=1)\n",
    "# masked_ce = mask * cross_entropy\n",
    "# masked_ce = tf.reshape(masked_ce, tf.shape(y))\n",
    "# mean_ce_by_example = tf.reduce_sum(masked_losses, axis=1) / example_len\n",
    "# mean_ce = tf.reduce_mean(mean_ce_by_example)\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)    \n",
    "    return train_op\n",
    "\n",
    "### Training\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 3\n",
    "DISPLAY_STEP = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAINING_SET_SIZE = 7\n",
    "\n",
    "filename = 'Sequence_labeling.tfr'\n",
    "with tf.Graph().as_default():\n",
    "    tf.set_random_seed(10)\n",
    "    np.random.seed(10)    \n",
    "    # Build Graph\n",
    "    lengths, sequences, labels = input_pipeline(filename, BATCH_SIZE)\n",
    "    logits, _ = inference(sequences, lengths)\n",
    "    avg_loss = loss(logits, labels, lengths)\n",
    "    train_op = training(avg_loss, LEARNING_RATE)\n",
    "    \n",
    "    # Create & Initialize Session\n",
    "    sess = tf.Session()\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # Start QueueRunner\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    try: \n",
    "        # Training cycles\n",
    "        for epoch in range(1, NUM_EPOCHS+1):\n",
    "            epoch_avg_loss = 0.0\n",
    "            total_batch = int(TRAINING_SET_SIZE / BATCH_SIZE\n",
    "                ) + 1 if TRAINING_SET_SIZE % BATCH_SIZE != 0 else int(\n",
    "                TRAINING_SET_SIZE / BATCH_SIZE)\n",
    "            for step in range(1, total_batch +1):\n",
    "                if coord.should_stop():\n",
    "                    break\n",
    "                _, train_loss = sess.run([train_op, avg_loss]) \n",
    "                epoch_avg_loss += train_loss / total_batch\n",
    "                assert not np.isnan(train_loss), 'Model diverged with loss = NaN'\n",
    "                \n",
    "                if step % DISPLAY_STEP == 0:\n",
    "                    print('%s: epoch %d, step %d, train_loss = %.6f'\n",
    "                        % (datetime.now(), epoch, step, train_loss))\n",
    "                \n",
    "            print('%s: epoch %d avg_loss = %.6f'\n",
    "                % (datetime.now(), epoch, epoch_avg_loss))\n",
    "                \n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        print(e.error_code, e.message)\n",
    "        print('Done!')\n",
    "    \n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "    \n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "    \n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.66392612  2.02395773  2.23962045  1.61670065  0.14862178  0.19047575\n",
      "  1.26339984  1.89619958  1.09861231]\n",
      "[ 0.66392612  2.02395773  2.23962045  1.61670065  0.14862178  0.19047575\n",
      "  1.26339984  1.89619958  0.        ]\n",
      "[[ 0.66392612  2.02395773  2.23962045]\n",
      " [ 1.61670065  0.14862178  0.19047575]\n",
      " [ 1.26339984  1.89619958  0.        ]]\n",
      "[ 1.64250147  0.65193272  1.57979965]\n",
      "1.291411280632019\n"
     ]
    }
   ],
   "source": [
    "## Illustration of intermediate results, run-time values excerpted from tfdbg\n",
    "tf.reset_default_graph()\n",
    "labels_flat = np.array([0,1,2,1,0,0,1,1,0])\n",
    "logits_flat = np.array([[0.6422022 , -0.08686169, -0.13423285],\n",
    "                        [0.88595915, -0.87825918, -1.19131315],\n",
    "                        [1.09363282, -0.92535424, -0.90869629],\n",
    "                        [1.34931934,  0.12072743, -0.35725975],\n",
    "                        [2.14791036, -0.19916281, -0.59185725],\n",
    "                        [1.07656074, -1.0128665 , -1.37606812],\n",
    "                        [0.41977212, -0.05744966, -0.13450697],\n",
    "                        [1.32846284, -0.24778995, -0.44169524],\n",
    "                        [0.1       ,  0.1       ,  0.1       ]], dtype=np.float32)\n",
    "losses_flat = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_flat, labels=labels_flat)\n",
    "\n",
    "actual_lengths = np.array([3,3,2])\n",
    "seq_mask_flat = np.array([1.,1.,1.,1.,1.,1.,1.,1.,0.], dtype=np.float32)\n",
    "masked_losses_flat = tf.multiply(seq_mask_flat, losses_flat)\n",
    "labels = tf.reshape(labels_flat, [3, -1])\n",
    "masked_losses = tf.reshape(masked_losses_flat, tf.shape(labels))\n",
    "mean_loss_by_example = tf.div(tf.reduce_sum(masked_losses, axis=1), tf.cast(actual_lengths, tf.float32))\n",
    "mean_loss = tf.reduce_mean(mean_loss_by_example)                            \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    a, b, c, d, e = sess.run([losses_flat, masked_losses_flat, masked_losses, mean_loss_by_example, mean_loss])\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "    print(d)\n",
    "    print(\"%.15f\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.0991137   1.43568242  2.01699328  0.20474197  1.0992794   3.00335884\n",
      "  0.09546375  1.0992794   1.0992794   1.0992794   0.19948083  2.47502112\n",
      "  2.28775692  2.53907585  0.22659162]\n",
      "[ 1.0991137   1.43568242  2.01699328  0.20474197  0.          3.00335884\n",
      "  0.09546375  0.          0.          0.          0.19948083  2.47502112\n",
      "  2.28775692  2.53907585  0.22659162]\n",
      "[[ 1.0991137   1.43568242  2.01699328  0.20474197  0.        ]\n",
      " [ 3.00335884  0.09546375  0.          0.          0.        ]\n",
      " [ 0.19948083  2.47502112  2.28775692  2.53907585  0.22659162]]\n",
      "[ 1.18913281  1.5494113   1.54558539]\n",
      "1.428043007850647\n"
     ]
    }
   ],
   "source": [
    "## Illustration of intermediate results, run-time values excerpted from tfdbg\n",
    "\n",
    "tf.reset_default_graph()\n",
    "labels_flat = np.array([2, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 2, 0])\n",
    "logits_flat = np.array([[0.09829693 , 0.1014218 ,  0.09910872],\n",
    "                        [0.49634132, -0.2791599 , -0.24879429],\n",
    "                        [0.57596284, -1.16146052, -1.34416914],\n",
    "                        [1.42855453, -0.80200392, -0.69387823],\n",
    "                        [0.099     ,  0.101     ,  0.09900001],\n",
    "                        [2.36766672, -0.05113668, -0.49951288],\n",
    "                        [1.89118695, -1.02813923, -1.18361068],\n",
    "                        [0.099     ,  0.101     ,  0.09900001],\n",
    "                        [0.099     ,  0.101     ,  0.09900001],\n",
    "                        [0.099     ,  0.101     ,  0.09900001],\n",
    "                        [1.59493768, -0.56841475, -0.65098649],\n",
    "                        [1.08355463, -1.22900999, -1.47534275],\n",
    "                        [1.36640537, -0.7257424 , -1.01273966],\n",
    "                        [1.60844588, -0.58185029, -0.74234509],\n",
    "                        [1.38049722, -0.61191964, -0.75697964],\n",
    "                       ], dtype=np.float32)\n",
    "losses_flat = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_flat, labels=labels_flat)\n",
    "\n",
    "actual_lengths = np.array([4,2,5])\n",
    "seq_mask_flat = np.array([1.,1.,1.,1.,0.,1.,1.,0.,0.,0.,1.,1.,1.,1.,1.], dtype=np.float32)\n",
    "masked_losses_flat = tf.multiply(seq_mask_flat, losses_flat)\n",
    "labels = tf.reshape(labels_flat, [3, -1])\n",
    "masked_losses = tf.reshape(masked_losses_flat, tf.shape(labels))\n",
    "mean_loss_by_example = tf.div(tf.reduce_sum(masked_losses, axis=1), tf.cast(actual_lengths, tf.float32))\n",
    "mean_loss = tf.reduce_mean(mean_loss_by_example)                            \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    a, b, c, d, e = sess.run([losses_flat, masked_losses_flat, masked_losses, mean_loss_by_example, mean_loss])\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "    print(d)\n",
    "    print(\"%.15f\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# References:\n",
    "# (1) http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\n",
    "# (2) https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/\n",
    "# (3) https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "# (4) https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py)\n",
    "# (5) https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/dynamic_rnn.py\n",
    "# (6) https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
