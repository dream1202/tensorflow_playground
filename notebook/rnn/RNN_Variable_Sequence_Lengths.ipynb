{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-58a5c13684e6>:67: run_n (from tensorflow.contrib.learn.python.learn.graph_actions) is deprecated and will be removed after 2017-02-15.\n",
      "Instructions for updating:\n",
      "graph_actions.py will be deleted. Use tf.train.* utilities instead. You can use learn/estimators/estimator.py as an example.\n",
      "WARNING:tensorflow:From /Users/Winston/.pyenv/versions/2.7.10/envs/tf-p27/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py:842: run_feeds (from tensorflow.contrib.learn.python.learn.graph_actions) is deprecated and will be removed after 2017-02-15.\n",
      "Instructions for updating:\n",
      "graph_actions.py will be deleted. Use tf.train.* utilities instead. You can use learn/estimators/estimator.py as an example.\n",
      "WARNING:tensorflow:From /Users/Winston/.pyenv/versions/2.7.10/envs/tf-p27/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py:900: run_feeds_iter (from tensorflow.contrib.learn.python.learn.graph_actions) is deprecated and will be removed after 2017-02-15.\n",
      "Instructions for updating:\n",
      "graph_actions.py will be deleted. Use tf.train.* utilities instead. You can use learn/estimators/estimator.py as an example.\n",
      "[[5 0 0 0 0 0 0 0]\n",
      " [7 5 0 0 0 0 0 0]\n",
      " [9 5 2 0 0 0 0 0]\n",
      " [2 5 3 7 8 9 9 3]]\n",
      "[5 0 0 0 0 0 0 0 7 5 0 0 0 0 0 0 9 5 2 0 0 0 0 0 2 5 3 7 8 9 9 3]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  1.  1.\n",
      "  1.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "[[ 24.29052544   0.           0.           0.           0.           0.\n",
      "    0.           0.        ]\n",
      " [ 13.50571346   7.59000778   0.           0.           0.           0.\n",
      "    0.           0.        ]\n",
      " [ 20.49732208   8.08126068  23.02852821   0.           0.           0.\n",
      "    0.           0.        ]\n",
      " [  8.94178772   9.2626791   16.82733917  35.03460693   5.39511824\n",
      "   22.45616722  18.60317993  19.56615067]]\n",
      "[ 24.29052544  10.5478611   17.20236969  17.01087952]\n",
      "17.2629\n"
     ]
    }
   ],
   "source": [
    "## SOLUTION I:\n",
    "####\n",
    "# USE label 0 to mask: actual label 0 shifted to 1, and num_class to be one size larger\n",
    "####\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf.set_random_seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "# Batch size\n",
    "B = 4 \n",
    "# Max number of time steps in a batch\n",
    "T = 8\n",
    "RNN_DIM = 128\n",
    "NUM_CLASS = 10 # actually 9 in use\n",
    "\n",
    "# Acutal length of examples\n",
    "example_len = [1,2,3,8]\n",
    "\n",
    "# The classes of the examples at each step (Classes 1 ~ 9 used, 0 means padding)\n",
    "y = np.random.randint(1, NUM_CLASS, [B, T])\n",
    "for i, length in enumerate(example_len):\n",
    "    y[i, length:] = 0\n",
    "\n",
    "# RNN outputs\n",
    "rnn_outputs = tf.convert_to_tensor(np.random.randn(B, T, RNN_DIM), dtype=tf.float32)\n",
    "\n",
    "# Output layer weights\n",
    "W = tf.get_variable(name=\"W\", initializer=tf.random_normal_initializer(), shape=[RNN_DIM, NUM_CLASS])\n",
    "\n",
    "# Calculate logits and probs\n",
    "# Reshape so we can calculate them all at once\n",
    "rnn_outputs_flat = tf.reshape(rnn_outputs, [-1, RNN_DIM])\n",
    "logits_flat = tf.matmul(rnn_outputs_flat, W)\n",
    "probs_flat = tf.nn.softmax(logits_flat)\n",
    "\n",
    "# Calculate the losses\n",
    "y_flat = tf.reshape(y, [-1])\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_flat, labels=y_flat)\n",
    "\n",
    "# Mask the losses via \n",
    "mask = tf.sign(tf.to_float(y_flat))\n",
    "masked_losses = mask * losses\n",
    "\n",
    "# Bring back to [B, T] shape\n",
    "masked_losses = tf.reshape(masked_losses, tf.shape(y))\n",
    "\n",
    "# Calculate mean loss\n",
    "mean_loss_by_example = tf.reduce_sum(masked_losses, axis=1) / example_len\n",
    "mean_loss = tf.reduce_mean(mean_loss_by_example)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     mean_loss = sess.run(mean_loss)\n",
    "#     print(mean_loss)\n",
    "    \n",
    "result = tf.contrib.learn.run_n(\n",
    "    {\n",
    "        \"y_flat\" : y_flat,\n",
    "        \"mask\" : mask,\n",
    "        \"masked_losses\": masked_losses,\n",
    "        \"mean_loss_by_example\": mean_loss_by_example,\n",
    "        \"mean_loss\": mean_loss\n",
    "    },\n",
    "    n=1,\n",
    "    feed_dict=None)\n",
    "\n",
    "print y\n",
    "print(result[0][\"y_flat\"])\n",
    "print(result[0][\"mask\"])\n",
    "print(result[0][\"masked_losses\"])\n",
    "print(result[0][\"mean_loss_by_example\"])\n",
    "print(result[0][\"mean_loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 1 loss: 16.384155 ; version 2 loss: 16.384155\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  1.  1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "## SOLUTION II:\n",
    "####\n",
    "# USE sequence_length to mask, independent of num_class & labels\n",
    "####\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf.set_random_seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "# Batch size\n",
    "B = 4 \n",
    "# Max number of time steps in a batch\n",
    "T = 8\n",
    "RNN_DIM = 128\n",
    "NUM_CLASS = 10\n",
    "\n",
    "# Acutal length of examples\n",
    "example_len = [1,2,3,8]\n",
    "\n",
    "\n",
    "# The classes of the examples at each step (Classes 0 ~ 9 used)\n",
    "y = np.random.randint(0, NUM_CLASS, [B, T])\n",
    "\n",
    "# RNN outputs (faked, simulating the outputs returned by tf.rnn.dynamic_rnn(...))\n",
    "rnn_outputs = tf.convert_to_tensor(np.random.randn(B, T, RNN_DIM), dtype=tf.float32)\n",
    "\n",
    "# Output layer weights\n",
    "W = tf.get_variable(name=\"W\", initializer=tf.random_normal_initializer(), shape=[RNN_DIM, NUM_CLASS])\n",
    "\n",
    "# Calculate logits and probs\n",
    "# Reshape so we can calculate them all at once\n",
    "rnn_outputs_flat = tf.reshape(rnn_outputs, [-1, RNN_DIM])\n",
    "logits_flat = tf.matmul(rnn_outputs_flat, W)\n",
    "probs_flat = tf.nn.softmax(logits_flat)\n",
    "probs = tf.reshape(probs_flat, [-1, T, NUM_CLASS])\n",
    "\n",
    "# Calculate the losses\n",
    "y_flat = tf.reshape(y, [-1])\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_flat, labels=y_flat)\n",
    "# Mask the losses via sequence length\n",
    "mask = tf.sequence_mask(example_len, T, dtype=tf.float32)\n",
    "mask = tf.reshape(mask, [-1])\n",
    "masked_losses = mask * losses\n",
    "\n",
    "\n",
    "# Bring back to [B, T] shape\n",
    "masked_losses = tf.reshape(masked_losses, tf.shape(y))\n",
    "# Calculate mean loss\n",
    "mean_loss_by_example = tf.reduce_sum(masked_losses, axis=1) / example_len\n",
    "mean_loss = tf.reduce_mean(mean_loss_by_example)\n",
    "\n",
    "\n",
    "# Calculate the losses ver 2\n",
    "cross_entropy = tf.one_hot(y_flat, NUM_CLASS) * tf.log(probs_flat)\n",
    "cross_entropy = -tf.reduce_sum(cross_entropy, axis=1)\n",
    "masked_ce = mask * cross_entropy\n",
    "masked_ce = tf.reshape(masked_ce, tf.shape(y))\n",
    "mean_ce_by_example = tf.reduce_sum(masked_losses, axis=1) / example_len\n",
    "mean_ce = tf.reduce_mean(mean_ce_by_example)\n",
    "\n",
    "\n",
    "# Calculate error\n",
    "errors = tf.not_equal(y_flat, tf.argmax(probs_flat, 1))\n",
    "errors = tf.cast(errors, tf.float32)\n",
    "masked_errors = mask * errors\n",
    "masked_errors = tf.reshape(masked_errors, tf.shape(y))\n",
    "mean_error_by_example = tf.reduce_sum(masked_errors, axis=1) / example_len\n",
    "mean_error = tf.reduce_mean(mean_error_by_example)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    mean_loss, masked_errors, mean_ce = sess.run([mean_loss, masked_errors, mean_ce])\n",
    "    print(\"version 1 loss: %f ; version 2 loss: %f\" %(mean_loss, mean_ce))\n",
    "    print(masked_errors)\n",
    "\n",
    "    \n",
    "# result = tf.contrib.learn.run_n(\n",
    "#     {\n",
    "#         \"y_flat\" : y_flat,\n",
    "#         \"mask\" : mask,\n",
    "#         \"masked_losses\": masked_losses,\n",
    "#         \"mean_loss_by_example\": mean_loss_by_example,\n",
    "#         \"mean_loss\": mean_loss,\n",
    "#         \"probs\": probs,\n",
    "#         \"predicts\": tf.argmax(probs,2),\n",
    "#         \"errors\": errors,\n",
    "#         \"masked_errors\": masked_errors,\n",
    "#         \"mean_error_by_example\": mean_error_by_example,\n",
    "#         \"mean_error\": mean_error,\n",
    "#         \"masked_ce\": masked_ce,\n",
    "#         \"mean_ce_by_example\": mean_ce_by_example,        \n",
    "#         \"mean_ce\": mean_ce\n",
    "#     },\n",
    "#     n=1,\n",
    "#     feed_dict=None)\n",
    "\n",
    "# print(result[0][\"y_flat\"])\n",
    "# print(result[0][\"mask\"])\n",
    "# print(result[0][\"masked_losses\"])\n",
    "# print(result[0][\"mean_loss_by_example\"])\n",
    "# print(result[0][\"mean_loss\"])\n",
    "# print (\"--------\")\n",
    "# print(result[0][\"probs\"].shape)\n",
    "# print(y)\n",
    "# print(result[0][\"predicts\"])\n",
    "# print(result[0][\"errors\"])\n",
    "# print(result[0][\"masked_errors\"])\n",
    "# print(result[0][\"mean_error_by_example\"])\n",
    "# print(result[0][\"mean_error\"])\n",
    "# print (\"--------\")\n",
    "# print(result[0][\"masked_ce\"])\n",
    "# print(result[0][\"mean_ce_by_example\"])\n",
    "# print(result[0][\"mean_ce\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notes:\n",
    "# (1) tf.nn.rnn was moved to tf.contrib.rnn.static_rnn(...), be noted that \n",
    "#     <1> its \"inputs\" is a length T list of inputs, which is in [batch_size, input_size] shape\n",
    "#     <2> its returned a pair of (outputs, state) where \n",
    "#         -- outputs is a length T list of outputs shaped in [batch_size, hidden_units]\n",
    "#         -- state is the final state\n",
    "# (2) tf.nn.dynamic_rnn(...): \n",
    "#     <1> it's \"inputs\" is a tensor of shape [batch_size, max_time_steps, input_features]\n",
    "#     <2> it returns a pair of (outputs, state) where \n",
    "#         -- outputs is a tensor shaped in [batch_size, max_time_step, hidden_units]\n",
    "#         -- state is the final state shaped in [batch_size, hidden_units]\n",
    "#\n",
    "\n",
    "# tensor in [batch_size, max_time_steps, input_features]\n",
    "def unstack_sequence(tensor):\n",
    "    \"\"\"Split the single tensor of a sequence into a list of frames.\"\"\"\n",
    "    return tf.unstack(tf.transpose(tensor, perm=[1, 0, 2]))\n",
    "\n",
    "def stack_sequence(sequence):\n",
    "    \"\"\"Combine a list of the frames into a single tensor of the sequence.\"\"\"\n",
    "    return tf.transpose(tf.stack(sequence), perm=[1, 0, 2])\n",
    "\n",
    "# (2) Assume equal-length inputs rather than variable-length inputs:\n",
    "#     To retrieve the output of the last time step (a batch unit):\n",
    "#\n",
    "#     output, _ = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)\n",
    "#     <i> earlier way\n",
    "#     output = tf.transpose(output, [1, 0, 2])\n",
    "#     last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "#     <ii> current way\n",
    "#     output = tf.transpose(output, [1, 0, 2])\n",
    "#     last = output[int(output.get_shape()[0]) - 1]\n",
    "\n",
    "\n",
    "# (3) Calculate actual timestep length of examples of a batch (examples may have different lengths)\n",
    "#     Note: It won't work if zero vectors could be actual input features\n",
    "# tensor in shape of [batch_size, max_time_steps, input_features]\n",
    "def length(tensor):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(tensor), axis=2))\n",
    "    length = tf.reduce_sum(used, axis=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "\n",
    "# (4) Get the last relevent output of variable time-step length examples in a batch\n",
    "# <1> Old way:\n",
    "def last_relevant(outputs, seq_lengths):\n",
    "    \"\"\"\n",
    "    :param outputs: [batch_size x max_seq_length x hidden_size] tensor of dynamic_rnn outputs\n",
    "    :param seq_lengths: [batch_size] tensor of sequence lengths\n",
    "    :return: [batch_size x hidden_size] tensor of last outputs\n",
    "    \"\"\"\n",
    "    batch_size, max_seq_length, hidden_size = tf.unpack(tf.shape(outputs))\n",
    "    index = tf.range(0, batch_size) * max_seq_length + (seq_lengths - 1)\n",
    "    return tf.gather(tf.reshape(outputs, [-1, hidden_size]), index)\n",
    "\n",
    "# <2> New way:\n",
    "def _last_relevant(outputs, actual_lengths):\n",
    "    \"\"\"\n",
    "    :param outputs: [batch_size x max_seq_length x hidden_size] tensor of dynamic_rnn outputs\n",
    "    :param actual_lengths: [batch_size] tensor of sequence actual lengths\n",
    "    :return: [batch_size x hidden_size] tensor of last outputs\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    return tf.gather_nd(outputs, tf.stack([tf.range(batch_size), actual_lengths - 1], axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# References:\n",
    "# (1) http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\n",
    "# (2) https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/\n",
    "# (3) https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "# (4) https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py)\n",
    "# (5) https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/dynamic_rnn.py\n",
    "# (6) https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
