{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Winston/.pyenv/versions/2.7.10/envs/tf-p27/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-25 11:23:35.529648: epoch 1 avg_loss = 1.119818\n",
      "2017-04-25 11:23:35.565268: epoch 2 avg_loss = 1.021896\n",
      "2017-04-25 11:23:35.602352: epoch 3 avg_loss = 1.091379\n",
      "2017-04-25 11:23:35.634904: epoch 4 avg_loss = 1.038721\n",
      "2017-04-25 11:23:35.667892: epoch 5 avg_loss = 1.073843\n",
      "2017-04-25 11:23:35.703451: epoch 6 avg_loss = 1.020319\n",
      "2017-04-25 11:23:35.746945: epoch 7 avg_loss = 1.036178\n",
      "2017-04-25 11:23:35.789090: epoch 8 avg_loss = 1.083859\n",
      "2017-04-25 11:23:35.826458: epoch 9 avg_loss = 0.989597\n",
      "2017-04-25 11:23:35.863207: epoch 10 avg_loss = 1.055611\n",
      "2017-04-25 11:23:35.898962: epoch 11 avg_loss = 1.006814\n",
      "2017-04-25 11:23:35.932592: epoch 12 avg_loss = 1.041140\n",
      "2017-04-25 11:23:35.970832: epoch 13 avg_loss = 0.991523\n",
      "2017-04-25 11:23:36.008010: epoch 14 avg_loss = 1.003261\n",
      "2017-04-25 11:23:36.033581: epoch 15 avg_loss = 1.053336\n",
      "2017-04-25 11:23:36.066421: epoch 16 avg_loss = 0.962615\n",
      "2017-04-25 11:23:36.100471: epoch 17 avg_loss = 1.024655\n",
      "2017-04-25 11:23:36.135380: epoch 18 avg_loss = 0.981028\n",
      "2017-04-25 11:23:36.164481: epoch 19 avg_loss = 1.013686\n",
      "2017-04-25 11:23:36.201993: epoch 20 avg_loss = 0.966608\n",
      "2017-04-25 11:23:36.241266: epoch 21 avg_loss = 0.976100\n",
      "2017-04-25 11:23:36.271106: epoch 22 avg_loss = 1.026778\n",
      "2017-04-25 11:23:36.309486: epoch 23 avg_loss = 0.939708\n",
      "2017-04-25 11:23:36.347807: epoch 24 avg_loss = 0.997188\n",
      "2017-04-25 11:23:36.382007: epoch 25 avg_loss = 0.959433\n",
      "2017-04-25 11:23:36.411038: epoch 26 avg_loss = 0.989939\n",
      "2017-04-25 11:23:36.438714: epoch 27 avg_loss = 0.943976\n",
      "2017-04-25 11:23:36.470788: epoch 28 avg_loss = 0.952461\n",
      "2017-04-25 11:23:36.503956: epoch 29 avg_loss = 1.003042\n",
      "2017-04-25 11:23:36.543105: epoch 30 avg_loss = 0.919179\n",
      "2017-04-25 11:23:36.584226: epoch 31 avg_loss = 0.971739\n",
      "2017-04-25 11:23:36.620945: epoch 32 avg_loss = 0.940390\n",
      "2017-04-25 11:23:36.660826: epoch 33 avg_loss = 0.968507\n",
      "2017-04-25 11:23:36.694992: epoch 34 avg_loss = 0.922392\n",
      "2017-04-25 11:23:36.732253: epoch 35 avg_loss = 0.930911\n",
      "2017-04-25 11:23:36.764991: epoch 36 avg_loss = 0.981070\n",
      "2017-04-25 11:23:36.803834: epoch 37 avg_loss = 0.899963\n",
      "2017-04-25 11:23:36.840660: epoch 38 avg_loss = 0.947343\n",
      "2017-04-25 11:23:36.876788: epoch 39 avg_loss = 0.922782\n",
      "2017-04-25 11:23:36.912565: epoch 40 avg_loss = 0.948523\n",
      "2017-04-25 11:23:36.943371: epoch 41 avg_loss = 0.901249\n",
      "2017-04-25 11:23:36.975343: epoch 42 avg_loss = 0.910722\n",
      "2017-04-25 11:23:37.007786: epoch 43 avg_loss = 0.960153\n",
      "2017-04-25 11:23:37.041852: epoch 44 avg_loss = 0.881592\n",
      "2017-04-25 11:23:37.075869: epoch 45 avg_loss = 0.923596\n",
      "2017-04-25 11:23:37.106477: epoch 46 avg_loss = 0.905984\n",
      "2017-04-25 11:23:37.135727: epoch 47 avg_loss = 0.929585\n",
      "2017-04-25 11:23:37.167295: epoch 48 avg_loss = 0.880499\n",
      "2017-04-25 11:23:37.198279: epoch 49 avg_loss = 0.891603\n",
      "2017-04-25 11:23:37.224035: epoch 50 avg_loss = 0.940053\n",
      "2017-04-25 11:23:37.258364: epoch 51 avg_loss = 0.863993\n",
      "2017-04-25 11:23:37.291245: epoch 52 avg_loss = 0.900676\n",
      "2017-04-25 11:23:37.324813: epoch 53 avg_loss = 0.889845\n",
      "2017-04-25 11:23:37.354428: epoch 54 avg_loss = 0.911677\n",
      "2017-04-25 11:23:37.388183: epoch 55 avg_loss = 0.860591\n",
      "2017-04-25 11:23:37.428692: epoch 56 avg_loss = 0.873563\n",
      "2017-04-25 11:23:37.461172: epoch 57 avg_loss = 0.920943\n",
      "2017-04-25 11:23:37.502899: epoch 58 avg_loss = 0.847338\n",
      "2017-04-25 11:23:37.548338: epoch 59 avg_loss = 0.879054\n",
      "2017-04-25 11:23:37.583854: epoch 60 avg_loss = 0.874512\n",
      "2017-04-25 11:23:37.618442: epoch 61 avg_loss = 0.894918\n",
      "2017-04-25 11:23:37.647899: epoch 62 avg_loss = 0.841943\n",
      "2017-04-25 11:23:37.684197: epoch 63 avg_loss = 0.856643\n",
      "2017-04-25 11:23:37.716893: epoch 64 avg_loss = 0.902991\n",
      "2017-04-25 11:23:37.753312: epoch 65 avg_loss = 0.831697\n",
      "2017-04-25 11:23:37.786522: epoch 66 avg_loss = 0.858846\n",
      "2017-04-25 11:23:37.816090: epoch 67 avg_loss = 0.860003\n",
      "2017-04-25 11:23:37.850003: epoch 68 avg_loss = 0.879232\n",
      "2017-04-25 11:23:37.885594: epoch 69 avg_loss = 0.824479\n",
      "2017-04-25 11:23:37.927271: epoch 70 avg_loss = 0.840690\n",
      "2017-04-25 11:23:37.964507: epoch 71 avg_loss = 0.886074\n",
      "2017-04-25 11:23:38.007329: epoch 72 avg_loss = 0.816911\n",
      "2017-04-25 11:23:38.050745: epoch 73 avg_loss = 0.839767\n",
      "2017-04-25 11:23:38.093288: epoch 74 avg_loss = 0.846113\n",
      "2017-04-25 11:23:38.132913: epoch 75 avg_loss = 0.864390\n",
      "2017-04-25 11:23:38.168440: epoch 76 avg_loss = 0.807893\n",
      "2017-04-25 11:23:38.205125: epoch 77 avg_loss = 0.825460\n",
      "2017-04-25 11:23:38.243668: epoch 78 avg_loss = 0.869923\n",
      "2017-04-25 11:23:38.302189: epoch 79 avg_loss = 0.802761\n",
      "2017-04-25 11:23:38.339711: epoch 80 avg_loss = 0.821504\n",
      "2017-04-25 11:23:38.373404: epoch 81 avg_loss = 0.832601\n",
      "2017-04-25 11:23:38.400211: epoch 82 avg_loss = 0.850188\n",
      "2017-04-25 11:23:38.429141: epoch 83 avg_loss = 0.791952\n",
      "2017-04-25 11:23:38.460511: epoch 84 avg_loss = 0.810744\n",
      "2017-04-25 11:23:38.489987: epoch 85 avg_loss = 0.854314\n",
      "2017-04-25 11:23:38.525853: epoch 86 avg_loss = 0.789084\n",
      "2017-04-25 11:23:38.554888: epoch 87 avg_loss = 0.803855\n",
      "2017-04-25 11:23:38.586502: epoch 88 avg_loss = 0.819300\n",
      "2017-04-25 11:23:38.614205: epoch 89 avg_loss = 0.836486\n",
      "2017-04-25 11:23:38.641543: epoch 90 avg_loss = 0.776534\n",
      "2017-04-25 11:23:38.673128: epoch 91 avg_loss = 0.796397\n",
      "2017-04-25 11:23:38.698472: epoch 92 avg_loss = 0.839112\n",
      "2017-04-25 11:23:38.733686: epoch 93 avg_loss = 0.775775\n",
      "2017-04-25 11:23:38.767570: epoch 94 avg_loss = 0.786711\n",
      "2017-04-25 11:23:38.796249: epoch 95 avg_loss = 0.806131\n",
      "2017-04-25 11:23:38.825010: epoch 96 avg_loss = 0.823197\n",
      "2017-04-25 11:23:38.855775: epoch 97 avg_loss = 0.761583\n",
      "2017-04-25 11:23:38.889644: epoch 98 avg_loss = 0.782328\n",
      "2017-04-25 11:23:38.921407: epoch 99 avg_loss = 0.824263\n",
      "2017-04-25 11:23:38.966698: epoch 100 avg_loss = 0.762769\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Sequence Labeling with variable-length sequences\n",
    "# Old approach to get the last relevant output for classification\n",
    "####\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "NUM_HIDDEN = 5\n",
    "NUM_CLASS = 3\n",
    "FEATURE_SIZE_PER_TIMESTEP = 5\n",
    "\n",
    "SEED = 10 # debugging & diagnostics purpose\n",
    "\n",
    "### Data pipeline\n",
    "def input_pipeline(filename, batch_size, epochs=None):\n",
    "    file_list = [os.path.join(os.getcwd(), filename)]\n",
    "    file_queue = tf.train.string_input_producer(file_list, num_epochs=epochs)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(file_queue)\n",
    "    sequence_features = {\n",
    "        \"inputs\": tf.FixedLenSequenceFeature([FEATURE_SIZE_PER_TIMESTEP], dtype=tf.float32),\n",
    "        \"label\": tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "    }\n",
    "    _, sequence = tf.parse_single_sequence_example(\n",
    "        serialized=serialized_example,\n",
    "        sequence_features=sequence_features)\n",
    "\n",
    "    actual_length = tf.shape(sequence[\"inputs\"])[0]\n",
    "    batch_lengths, batch_sequences, batch_labels = tf.train.batch(\n",
    "        [actual_length, sequence[\"inputs\"], sequence[\"label\"]],\n",
    "        batch_size=batch_size,\n",
    "        dynamic_pad=True,\n",
    "        allow_smaller_final_batch=True,\n",
    "        name=\"input_batching\")\n",
    "    return batch_lengths, batch_sequences, batch_labels\n",
    "\n",
    "\n",
    "def _last_relevant(outputs, actual_lengths):\n",
    "    \"\"\"\n",
    "    :param outputs: [batch_size x max_seq_length x hidden_size] tensor of dynamic_rnn outputs\n",
    "    :param actual_lengths: [batch_size] tensor of sequence actual lengths\n",
    "    :return: [batch_size x hidden_size] tensor of last outputs\n",
    "    \"\"\"\n",
    "    batch_size, max_seq_length, hidden_size = tf.unstack(tf.shape(outputs))\n",
    "    index = tf.range(0, batch_size) * max_seq_length + (actual_lengths - 1)\n",
    "    return tf.gather(tf.reshape(outputs, [-1, hidden_size]), index)\n",
    "\n",
    "\n",
    "### Build Model\n",
    "def inference(inputs, actual_lengths):\n",
    "    cell = tf.contrib.rnn.LSTMCell(NUM_HIDDEN, initializer=tf.truncated_normal_initializer(seed=SEED))\n",
    "    outputs, current_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, sequence_length=actual_lengths)\n",
    "    last_outputs = _last_relevant(outputs, actual_lengths)\n",
    "    # Output layer weights & biases\n",
    "    weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_CLASS], seed=SEED), dtype=tf.float32)\n",
    "    biases = tf.Variable(tf.constant(0.1, shape=[NUM_CLASS]), dtype=tf.float32)\n",
    "    # Softmax classification based on outputs of the last time step of each sequence\n",
    "    logits = tf.add(tf.matmul(last_outputs, weights), biases)\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    return logits, predictions\n",
    "\n",
    "\n",
    "## Cost function\n",
    "def loss(logits, labels, actual_lengths):\n",
    "    labels_flat = tf.reshape(labels, [-1])\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_flat)\n",
    "    mean_loss = tf.reduce_mean(cross_entropy)\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "## Error tracking \n",
    "def error(predictions, labels, actual_lengths):\n",
    "    labels_flat = tf.reshape(labels, [-1])\n",
    "    errors = tf.not_equal(tf.argmax(predictions, 1))\n",
    "    mean_error = tf.reduce_mean(tf.cast(errors, tf.float32))\n",
    "    return mean_error\n",
    "    \n",
    "\n",
    "def training(loss, learning_rate):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)    \n",
    "    return train_op\n",
    "\n",
    "\n",
    "### Training\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 3\n",
    "DISPLAY_STEP = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAINING_SET_SIZE = 7\n",
    "\n",
    "filename = 'Sequence_classification.tfr'\n",
    "with tf.Graph().as_default():\n",
    "    tf.set_random_seed(SEED)\n",
    "    np.random.seed(SEED)    \n",
    "    # Build Graph\n",
    "    lengths, sequences, labels = input_pipeline(filename, BATCH_SIZE)\n",
    "    logits, _ = inference(sequences, lengths)\n",
    "    avg_loss = loss(logits, labels, lengths)\n",
    "    train_op = training(avg_loss, LEARNING_RATE)\n",
    "    \n",
    "    # Create & Initialize Session\n",
    "    sess = tf.Session()\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # Start QueueRunner\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    try: \n",
    "        # Training cycles\n",
    "        for epoch in range(1, NUM_EPOCHS+1):\n",
    "            epoch_avg_loss = 0.0\n",
    "            total_batch = int(TRAINING_SET_SIZE / BATCH_SIZE\n",
    "                ) + 1 if TRAINING_SET_SIZE % BATCH_SIZE != 0 else int(\n",
    "                TRAINING_SET_SIZE / BATCH_SIZE)\n",
    "            for step in range(1, total_batch +1):\n",
    "                if coord.should_stop():\n",
    "                    break\n",
    "                _, train_loss = sess.run([train_op, avg_loss]) \n",
    "                epoch_avg_loss += train_loss / total_batch\n",
    "                assert not np.isnan(train_loss), 'Model diverged with loss = NaN'\n",
    "                \n",
    "                if step % DISPLAY_STEP == 0:\n",
    "                    print('%s: epoch %d, step %d, train_loss = %.6f'\n",
    "                        % (datetime.now(), epoch, step, train_loss))\n",
    "                \n",
    "            print('%s: epoch %d avg_loss = %.6f'\n",
    "                % (datetime.now(), epoch, epoch_avg_loss))\n",
    "                \n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        print(e.error_code, e.message)\n",
    "        print('Done!')\n",
    "    \n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "    \n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "    \n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-cb5afeed17b0>:38: run_n (from tensorflow.contrib.learn.python.learn.graph_actions) is deprecated and will be removed after 2017-02-15.\n",
      "Instructions for updating:\n",
      "graph_actions.py will be deleted. Use tf.train.* utilities instead. You can use learn/estimators/estimator.py as an example.\n",
      "WARNING:tensorflow:From /Users/Winston/.pyenv/versions/2.7.10/envs/tf-p27/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py:842: run_feeds (from tensorflow.contrib.learn.python.learn.graph_actions) is deprecated and will be removed after 2017-02-15.\n",
      "Instructions for updating:\n",
      "graph_actions.py will be deleted. Use tf.train.* utilities instead. You can use learn/estimators/estimator.py as an example.\n",
      "WARNING:tensorflow:From /Users/Winston/.pyenv/versions/2.7.10/envs/tf-p27/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py:900: run_feeds_iter (from tensorflow.contrib.learn.python.learn.graph_actions) is deprecated and will be removed after 2017-02-15.\n",
      "Instructions for updating:\n",
      "graph_actions.py will be deleted. Use tf.train.* utilities instead. You can use learn/estimators/estimator.py as an example.\n",
      "[2 5 7]\n",
      "[[ -6.59987450e-01   3.52891497e-02  -8.80478648e-04  -2.64588028e-01\n",
      "    2.42421087e-02]\n",
      " [ -1.13629685e-04   4.00667477e-06  -4.22743142e-07  -1.92035735e-01\n",
      "    7.23191023e-01]\n",
      " [ -3.35084200e-02   7.04476086e-04  -1.08449552e-04  -2.25155890e-01\n",
      "    4.65190321e-01]]\n",
      "[[0 2]\n",
      " [1 2]\n",
      " [2 1]]\n",
      "[[ -6.59987450e-01   3.52891497e-02  -8.80478648e-04  -2.64588028e-01\n",
      "    2.42421087e-02]\n",
      " [ -1.13629685e-04   4.00667477e-06  -4.22743142e-07  -1.92035735e-01\n",
      "    7.23191023e-01]\n",
      " [ -3.35084200e-02   7.04476086e-04  -1.08449552e-04  -2.25155890e-01\n",
      "    4.65190321e-01]]\n"
     ]
    }
   ],
   "source": [
    "outputs = np.array([[[-1.11184277e-01, 2.33777296e-02, -5.76924253e-03, -2.03809544e-01, 2.85978884e-01],\n",
    "                     [-6.20732462e-05, 2.37267528e-07, -3.32064616e-08, -1.00004703e-01, 6.70888603e-01],\n",
    "                     [-6.59987450e-01, 3.52891497e-02, -8.80478648e-04, -2.64588028e-01, 2.42421087e-02]],\n",
    "                    \n",
    "                    [[-1.14256974e-04, 1.39966013e-03, -1.43084544e-05, -3.55578102e-02, 4.55059946e-01],\n",
    "                     [-1.29182416e-03, 1.95695069e-08, -2.30110445e-11, -3.71065177e-02, 5.85010469e-01],\n",
    "                     [-1.13629685e-04, 4.00667477e-06, -4.22743142e-07, -1.92035735e-01, 7.23191023e-01]],\n",
    "                    \n",
    "                    [[ 3.30646448e-02, 8.29776973e-02, -9.03536081e-02, -2.44489789e-01, 2.69344389e-01],\n",
    "                     [-3.35084200e-02, 7.04476086e-04, -1.08449552e-04, -2.25155890e-01, 4.65190321e-01],\n",
    "                     [ 0.00000000e+00, 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, 0.00000000e+00]]])\n",
    "actual_lengths = np.array([3,3,2], dtype=np.int64)\n",
    "\n",
    "batch_size, max_seq_length, hidden_size = tf.unstack(tf.shape(outputs))\n",
    "\n",
    "## (1) Old way:\n",
    "##     tf.gatehr(...) to gather last output via flatten outputs & 1d index array \n",
    "##     NOTE: This approach works, but throws a UserWarning as below,\n",
    "##         UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. \n",
    "##                      This may consume a large amount of memory.\n",
    "##                      \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
    "index_flat = tf.range(0, batch_size) * max_seq_length + (actual_lengths - 1)\n",
    "out_1 = tf.gather(tf.reshape(outputs, [-1, hidden_size]), index_flat)\n",
    "\n",
    "## (2) New way:\n",
    "##     tf.gather_nd(...) to gather last output via 2d index array directly\n",
    "index_nd = tf.stack([tf.range(0, batch_size), (actual_lengths - 1)], axis=1)\n",
    "out_2 = tf.gather_nd(outputs, index_nd)\n",
    "\n",
    "result = tf.contrib.learn.run_n(\n",
    "    {\n",
    "        \"index_flat\" : index_flat,\n",
    "        \"out_1\": out_1,\n",
    "        \"index_nd\" : index_nd,\n",
    "        \"out_2\" : out_2,\n",
    "    },\n",
    "    n=1,\n",
    "    feed_dict=None)\n",
    "\n",
    "print(result[0][\"index_flat\"])\n",
    "print(result[0][\"out_1\"])\n",
    "print(result[0][\"index_nd\"])\n",
    "print(result[0][\"out_2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-25 11:23:41.616083: epoch 1 avg_loss = 1.119818\n",
      "2017-04-25 11:23:41.654573: epoch 2 avg_loss = 1.021896\n",
      "2017-04-25 11:23:41.690342: epoch 3 avg_loss = 1.091379\n",
      "2017-04-25 11:23:41.723877: epoch 4 avg_loss = 1.038721\n",
      "2017-04-25 11:23:41.758106: epoch 5 avg_loss = 1.073843\n",
      "2017-04-25 11:23:41.791471: epoch 6 avg_loss = 1.020319\n",
      "2017-04-25 11:23:41.832775: epoch 7 avg_loss = 1.036178\n",
      "2017-04-25 11:23:41.873665: epoch 8 avg_loss = 1.083859\n",
      "2017-04-25 11:23:41.915846: epoch 9 avg_loss = 0.989597\n",
      "2017-04-25 11:23:41.962309: epoch 10 avg_loss = 1.055611\n",
      "2017-04-25 11:23:42.007428: epoch 11 avg_loss = 1.006814\n",
      "2017-04-25 11:23:42.044389: epoch 12 avg_loss = 1.041140\n",
      "2017-04-25 11:23:42.073556: epoch 13 avg_loss = 0.991523\n",
      "2017-04-25 11:23:42.109219: epoch 14 avg_loss = 1.003261\n",
      "2017-04-25 11:23:42.136669: epoch 15 avg_loss = 1.053336\n",
      "2017-04-25 11:23:42.176539: epoch 16 avg_loss = 0.962615\n",
      "2017-04-25 11:23:42.207938: epoch 17 avg_loss = 1.024655\n",
      "2017-04-25 11:23:42.242223: epoch 18 avg_loss = 0.981028\n",
      "2017-04-25 11:23:42.278785: epoch 19 avg_loss = 1.013686\n",
      "2017-04-25 11:23:42.309822: epoch 20 avg_loss = 0.966608\n",
      "2017-04-25 11:23:42.343723: epoch 21 avg_loss = 0.976100\n",
      "2017-04-25 11:23:42.370379: epoch 22 avg_loss = 1.026778\n",
      "2017-04-25 11:23:42.405483: epoch 23 avg_loss = 0.939708\n",
      "2017-04-25 11:23:42.442170: epoch 24 avg_loss = 0.997188\n",
      "2017-04-25 11:23:42.472951: epoch 25 avg_loss = 0.959433\n",
      "2017-04-25 11:23:42.515307: epoch 26 avg_loss = 0.989939\n",
      "2017-04-25 11:23:42.560718: epoch 27 avg_loss = 0.943976\n",
      "2017-04-25 11:23:42.611580: epoch 28 avg_loss = 0.952461\n",
      "2017-04-25 11:23:42.639826: epoch 29 avg_loss = 1.003042\n",
      "2017-04-25 11:23:42.677037: epoch 30 avg_loss = 0.919179\n",
      "2017-04-25 11:23:42.710885: epoch 31 avg_loss = 0.971739\n",
      "2017-04-25 11:23:42.739958: epoch 32 avg_loss = 0.940390\n",
      "2017-04-25 11:23:42.772248: epoch 33 avg_loss = 0.968507\n",
      "2017-04-25 11:23:42.802498: epoch 34 avg_loss = 0.922392\n",
      "2017-04-25 11:23:42.836106: epoch 35 avg_loss = 0.930911\n",
      "2017-04-25 11:23:42.867502: epoch 36 avg_loss = 0.981070\n",
      "2017-04-25 11:23:42.898459: epoch 37 avg_loss = 0.899963\n",
      "2017-04-25 11:23:42.928044: epoch 38 avg_loss = 0.947343\n",
      "2017-04-25 11:23:42.957432: epoch 39 avg_loss = 0.922782\n",
      "2017-04-25 11:23:42.986149: epoch 40 avg_loss = 0.948523\n",
      "2017-04-25 11:23:43.020807: epoch 41 avg_loss = 0.901249\n",
      "2017-04-25 11:23:43.072092: epoch 42 avg_loss = 0.910722\n",
      "2017-04-25 11:23:43.116141: epoch 43 avg_loss = 0.960153\n",
      "2017-04-25 11:23:43.155219: epoch 44 avg_loss = 0.881592\n",
      "2017-04-25 11:23:43.200617: epoch 45 avg_loss = 0.923596\n",
      "2017-04-25 11:23:43.232249: epoch 46 avg_loss = 0.905984\n",
      "2017-04-25 11:23:43.260466: epoch 47 avg_loss = 0.929585\n",
      "2017-04-25 11:23:43.291377: epoch 48 avg_loss = 0.880499\n",
      "2017-04-25 11:23:43.329535: epoch 49 avg_loss = 0.891603\n",
      "2017-04-25 11:23:43.358868: epoch 50 avg_loss = 0.940053\n",
      "2017-04-25 11:23:43.391605: epoch 51 avg_loss = 0.863993\n",
      "2017-04-25 11:23:43.422495: epoch 52 avg_loss = 0.900676\n",
      "2017-04-25 11:23:43.451627: epoch 53 avg_loss = 0.889845\n",
      "2017-04-25 11:23:43.480831: epoch 54 avg_loss = 0.911677\n",
      "2017-04-25 11:23:43.511603: epoch 55 avg_loss = 0.860591\n",
      "2017-04-25 11:23:43.549546: epoch 56 avg_loss = 0.873563\n",
      "2017-04-25 11:23:43.581214: epoch 57 avg_loss = 0.920943\n",
      "2017-04-25 11:23:43.616680: epoch 58 avg_loss = 0.847338\n",
      "2017-04-25 11:23:43.653819: epoch 59 avg_loss = 0.879054\n",
      "2017-04-25 11:23:43.687733: epoch 60 avg_loss = 0.874512\n",
      "2017-04-25 11:23:43.720994: epoch 61 avg_loss = 0.894918\n",
      "2017-04-25 11:23:43.750414: epoch 62 avg_loss = 0.841943\n",
      "2017-04-25 11:23:43.783858: epoch 63 avg_loss = 0.856643\n",
      "2017-04-25 11:23:43.811562: epoch 64 avg_loss = 0.902991\n",
      "2017-04-25 11:23:43.839836: epoch 65 avg_loss = 0.831697\n",
      "2017-04-25 11:23:43.869170: epoch 66 avg_loss = 0.858846\n",
      "2017-04-25 11:23:43.904959: epoch 67 avg_loss = 0.860003\n",
      "2017-04-25 11:23:43.947670: epoch 68 avg_loss = 0.879232\n",
      "2017-04-25 11:23:43.993765: epoch 69 avg_loss = 0.824479\n",
      "2017-04-25 11:23:44.031598: epoch 70 avg_loss = 0.840690\n",
      "2017-04-25 11:23:44.061705: epoch 71 avg_loss = 0.886074\n",
      "2017-04-25 11:23:44.093972: epoch 72 avg_loss = 0.816911\n",
      "2017-04-25 11:23:44.128442: epoch 73 avg_loss = 0.839767\n",
      "2017-04-25 11:23:44.159750: epoch 74 avg_loss = 0.846113\n",
      "2017-04-25 11:23:44.191440: epoch 75 avg_loss = 0.864390\n",
      "2017-04-25 11:23:44.221815: epoch 76 avg_loss = 0.807893\n",
      "2017-04-25 11:23:44.256690: epoch 77 avg_loss = 0.825460\n",
      "2017-04-25 11:23:44.287071: epoch 78 avg_loss = 0.869923\n",
      "2017-04-25 11:23:44.320875: epoch 79 avg_loss = 0.802761\n",
      "2017-04-25 11:23:44.352106: epoch 80 avg_loss = 0.821504\n",
      "2017-04-25 11:23:44.396479: epoch 81 avg_loss = 0.832601\n",
      "2017-04-25 11:23:44.447432: epoch 82 avg_loss = 0.850188\n",
      "2017-04-25 11:23:44.480835: epoch 83 avg_loss = 0.791952\n",
      "2017-04-25 11:23:44.516197: epoch 84 avg_loss = 0.810744\n",
      "2017-04-25 11:23:44.547028: epoch 85 avg_loss = 0.854314\n",
      "2017-04-25 11:23:44.574427: epoch 86 avg_loss = 0.789084\n",
      "2017-04-25 11:23:44.608367: epoch 87 avg_loss = 0.803855\n",
      "2017-04-25 11:23:44.640426: epoch 88 avg_loss = 0.819300\n",
      "2017-04-25 11:23:44.670244: epoch 89 avg_loss = 0.836486\n",
      "2017-04-25 11:23:44.698542: epoch 90 avg_loss = 0.776534\n",
      "2017-04-25 11:23:44.731183: epoch 91 avg_loss = 0.796397\n",
      "2017-04-25 11:23:44.763543: epoch 92 avg_loss = 0.839112\n",
      "2017-04-25 11:23:44.802236: epoch 93 avg_loss = 0.775775\n",
      "2017-04-25 11:23:44.842494: epoch 94 avg_loss = 0.786711\n",
      "2017-04-25 11:23:44.880211: epoch 95 avg_loss = 0.806131\n",
      "2017-04-25 11:23:44.910624: epoch 96 avg_loss = 0.823197\n",
      "2017-04-25 11:23:44.942343: epoch 97 avg_loss = 0.761583\n",
      "2017-04-25 11:23:44.988508: epoch 98 avg_loss = 0.782328\n",
      "2017-04-25 11:23:45.020288: epoch 99 avg_loss = 0.824263\n",
      "2017-04-25 11:23:45.054609: epoch 100 avg_loss = 0.762769\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Sequence Labeling with variable-length sequences\n",
    "# New approach to get the last relevant output for classification\n",
    "####\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "NUM_HIDDEN = 5\n",
    "NUM_CLASS = 3\n",
    "FEATURE_SIZE_PER_TIMESTEP = 5\n",
    "\n",
    "SEED = 10 # debugging & diagnostics purpose\n",
    "\n",
    "### Data pipeline\n",
    "def input_pipeline(filename, batch_size, epochs=None):\n",
    "    file_list = [os.path.join(os.getcwd(), filename)]\n",
    "    file_queue = tf.train.string_input_producer(file_list, num_epochs=epochs)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(file_queue)\n",
    "    sequence_features = {\n",
    "        \"inputs\": tf.FixedLenSequenceFeature([FEATURE_SIZE_PER_TIMESTEP], dtype=tf.float32),\n",
    "        \"label\": tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "    }\n",
    "    _, sequence = tf.parse_single_sequence_example(\n",
    "        serialized=serialized_example,\n",
    "        sequence_features=sequence_features)\n",
    "\n",
    "    actual_length = tf.shape(sequence[\"inputs\"])[0]\n",
    "    batch_lengths, batch_sequences, batch_labels = tf.train.batch(\n",
    "        [actual_length, sequence[\"inputs\"], sequence[\"label\"]],\n",
    "        batch_size=batch_size,\n",
    "        dynamic_pad=True,\n",
    "        allow_smaller_final_batch=True,\n",
    "        name=\"input_batching\")\n",
    "    return batch_lengths, batch_sequences, batch_labels\n",
    "\n",
    "\n",
    "\n",
    "def _last_relevant(outputs, actual_lengths):\n",
    "    \"\"\"\n",
    "    :param outputs: [batch_size x max_seq_length x hidden_size] tensor of dynamic_rnn outputs\n",
    "    :param actual_lengths: [batch_size] tensor of sequence actual lengths\n",
    "    :return: [batch_size x hidden_size] tensor of last outputs\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    return tf.gather_nd(outputs, tf.stack([tf.range(batch_size), actual_lengths - 1], axis=1))\n",
    "\n",
    "\n",
    "### Build Model\n",
    "def inference(inputs, actual_lengths):\n",
    "    cell = tf.contrib.rnn.LSTMCell(NUM_HIDDEN, initializer=tf.truncated_normal_initializer(seed=SEED))\n",
    "    outputs, current_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, sequence_length=actual_lengths)\n",
    "    last_outputs = _last_relevant(outputs, actual_lengths)\n",
    "    # Output layer weights & biases\n",
    "    weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_CLASS], seed=SEED), dtype=tf.float32)\n",
    "    biases = tf.Variable(tf.constant(0.1, shape=[NUM_CLASS]), dtype=tf.float32)\n",
    "    # Softmax classification based on outputs of the last time step of each sequence\n",
    "    logits = tf.add(tf.matmul(last_outputs, weights), biases)\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    return logits, predictions\n",
    "\n",
    "\n",
    "## Cost function\n",
    "def loss(logits, labels, actual_lengths):\n",
    "    labels_flat = tf.reshape(labels, [-1])\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_flat)\n",
    "    mean_loss = tf.reduce_mean(cross_entropy)\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "## Error tracking \n",
    "def error(predictions, labels, actual_lengths):\n",
    "    labels_flat = tf.reshape(labels, [-1])\n",
    "    errors = tf.not_equal(tf.argmax(predictions, 1))\n",
    "    mean_error = tf.reduce_mean(tf.cast(errors, tf.float32))\n",
    "    return mean_error\n",
    "    \n",
    "\n",
    "def training(loss, learning_rate):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)    \n",
    "    return train_op\n",
    "\n",
    "\n",
    "### Training\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 3\n",
    "DISPLAY_STEP = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAINING_SET_SIZE = 7\n",
    "\n",
    "filename = 'Sequence_classification.tfr'\n",
    "with tf.Graph().as_default():\n",
    "    tf.set_random_seed(SEED)\n",
    "    np.random.seed(SEED)    \n",
    "    # Build Graph\n",
    "    lengths, sequences, labels = input_pipeline(filename, BATCH_SIZE)\n",
    "    logits, _ = inference(sequences, lengths)\n",
    "    avg_loss = loss(logits, labels, lengths)\n",
    "    train_op = training(avg_loss, LEARNING_RATE)\n",
    "    \n",
    "    # Create & Initialize Session\n",
    "    sess = tf.Session()\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # Start QueueRunner\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    try: \n",
    "        # Training cycles\n",
    "        for epoch in range(1, NUM_EPOCHS+1):\n",
    "            epoch_avg_loss = 0.0\n",
    "            total_batch = int(TRAINING_SET_SIZE / BATCH_SIZE\n",
    "                ) + 1 if TRAINING_SET_SIZE % BATCH_SIZE != 0 else int(\n",
    "                TRAINING_SET_SIZE / BATCH_SIZE)\n",
    "            for step in range(1, total_batch +1):\n",
    "                if coord.should_stop():\n",
    "                    break\n",
    "                _, train_loss = sess.run([train_op, avg_loss]) \n",
    "                epoch_avg_loss += train_loss / total_batch\n",
    "                assert not np.isnan(train_loss), 'Model diverged with loss = NaN'\n",
    "                \n",
    "                if step % DISPLAY_STEP == 0:\n",
    "                    print('%s: epoch %d, step %d, train_loss = %.6f'\n",
    "                        % (datetime.now(), epoch, step, train_loss))\n",
    "                \n",
    "            print('%s: epoch %d avg_loss = %.6f'\n",
    "                % (datetime.now(), epoch, epoch_avg_loss))\n",
    "                \n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        print(e.error_code, e.message)\n",
    "        print('Done!')\n",
    "    \n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "    \n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "    \n",
    "print('Finished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
