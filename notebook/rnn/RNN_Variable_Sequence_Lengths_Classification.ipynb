{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Winston/.pyenv/versions/2.7.10/envs/tf-p27/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-24 14:40:02.554814: epoch 1 avg_loss = 1.408762\n",
      "2017-04-24 14:40:02.599809: epoch 2 avg_loss = 1.509718\n",
      "2017-04-24 14:40:02.641374: epoch 3 avg_loss = 1.442610\n",
      "2017-04-24 14:40:02.673496: epoch 4 avg_loss = 1.424193\n",
      "2017-04-24 14:40:02.706045: epoch 5 avg_loss = 1.407049\n",
      "2017-04-24 14:40:02.738983: epoch 6 avg_loss = 1.450705\n",
      "2017-04-24 14:40:02.779327: epoch 7 avg_loss = 1.373886\n",
      "2017-04-24 14:40:02.811267: epoch 8 avg_loss = 1.275502\n",
      "2017-04-24 14:40:02.846800: epoch 9 avg_loss = 1.376903\n",
      "2017-04-24 14:40:02.894188: epoch 10 avg_loss = 1.300501\n",
      "2017-04-24 14:40:02.927664: epoch 11 avg_loss = 1.309860\n",
      "2017-04-24 14:40:02.960635: epoch 12 avg_loss = 1.257352\n",
      "2017-04-24 14:40:02.996389: epoch 13 avg_loss = 1.319000\n",
      "2017-04-24 14:40:03.046167: epoch 14 avg_loss = 1.227976\n",
      "2017-04-24 14:40:03.109553: epoch 15 avg_loss = 1.151765\n",
      "2017-04-24 14:40:03.160921: epoch 16 avg_loss = 1.243197\n",
      "2017-04-24 14:40:03.208392: epoch 17 avg_loss = 1.163385\n",
      "2017-04-24 14:40:03.258136: epoch 18 avg_loss = 1.191237\n",
      "2017-04-24 14:40:03.292825: epoch 19 avg_loss = 1.108768\n",
      "2017-04-24 14:40:03.322832: epoch 20 avg_loss = 1.185251\n",
      "2017-04-24 14:40:03.355009: epoch 21 avg_loss = 1.084912\n",
      "2017-04-24 14:40:03.384876: epoch 22 avg_loss = 1.033537\n",
      "2017-04-24 14:40:03.415955: epoch 23 avg_loss = 1.110818\n",
      "2017-04-24 14:40:03.443914: epoch 24 avg_loss = 1.036803\n",
      "2017-04-24 14:40:03.472650: epoch 25 avg_loss = 1.050780\n",
      "2017-04-24 14:40:03.506006: epoch 26 avg_loss = 0.961570\n",
      "2017-04-24 14:40:03.537355: epoch 27 avg_loss = 1.039227\n",
      "2017-04-24 14:40:03.573732: epoch 28 avg_loss = 0.890679\n",
      "2017-04-24 14:40:03.604650: epoch 29 avg_loss = 0.896554\n",
      "2017-04-24 14:40:03.641621: epoch 30 avg_loss = 0.941742\n",
      "2017-04-24 14:40:03.676582: epoch 31 avg_loss = 0.888322\n",
      "2017-04-24 14:40:03.708521: epoch 32 avg_loss = 0.833677\n",
      "2017-04-24 14:40:03.739987: epoch 33 avg_loss = 0.825471\n",
      "2017-04-24 14:40:03.772677: epoch 34 avg_loss = 0.899795\n",
      "2017-04-24 14:40:03.807493: epoch 35 avg_loss = 0.736933\n",
      "2017-04-24 14:40:03.837002: epoch 36 avg_loss = 0.813244\n",
      "2017-04-24 14:40:03.878604: epoch 37 avg_loss = 0.852999\n",
      "2017-04-24 14:40:03.916888: epoch 38 avg_loss = 0.803963\n",
      "2017-04-24 14:40:03.948248: epoch 39 avg_loss = 0.764428\n",
      "2017-04-24 14:40:03.979330: epoch 40 avg_loss = 0.774634\n",
      "2017-04-24 14:40:04.012520: epoch 41 avg_loss = 0.829707\n",
      "2017-04-24 14:40:04.048583: epoch 42 avg_loss = 0.688543\n",
      "2017-04-24 14:40:04.078400: epoch 43 avg_loss = 0.755735\n",
      "2017-04-24 14:40:04.115224: epoch 44 avg_loss = 0.807575\n",
      "2017-04-24 14:40:04.150139: epoch 45 avg_loss = 0.751417\n",
      "2017-04-24 14:40:04.182980: epoch 46 avg_loss = 0.723201\n",
      "2017-04-24 14:40:04.215240: epoch 47 avg_loss = 0.738301\n",
      "2017-04-24 14:40:04.248212: epoch 48 avg_loss = 0.779907\n",
      "2017-04-24 14:40:04.284544: epoch 49 avg_loss = 0.652786\n",
      "2017-04-24 14:40:04.314689: epoch 50 avg_loss = 0.711374\n",
      "2017-04-24 14:40:04.352098: epoch 51 avg_loss = 0.768258\n",
      "2017-04-24 14:40:04.386664: epoch 52 avg_loss = 0.711225\n",
      "2017-04-24 14:40:04.419359: epoch 53 avg_loss = 0.686277\n",
      "2017-04-24 14:40:04.451643: epoch 54 avg_loss = 0.704666\n",
      "2017-04-24 14:40:04.482840: epoch 55 avg_loss = 0.737540\n",
      "2017-04-24 14:40:04.518504: epoch 56 avg_loss = 0.618642\n",
      "2017-04-24 14:40:04.548307: epoch 57 avg_loss = 0.672906\n",
      "2017-04-24 14:40:04.588755: epoch 58 avg_loss = 0.730524\n",
      "2017-04-24 14:40:04.623881: epoch 59 avg_loss = 0.675308\n",
      "2017-04-24 14:40:04.656437: epoch 60 avg_loss = 0.650773\n",
      "2017-04-24 14:40:04.689371: epoch 61 avg_loss = 0.672783\n",
      "2017-04-24 14:40:04.722598: epoch 62 avg_loss = 0.698284\n",
      "2017-04-24 14:40:04.759776: epoch 63 avg_loss = 0.585962\n",
      "2017-04-24 14:40:04.789956: epoch 64 avg_loss = 0.637335\n",
      "2017-04-24 14:40:04.827163: epoch 65 avg_loss = 0.694453\n",
      "2017-04-24 14:40:04.862308: epoch 66 avg_loss = 0.640287\n",
      "2017-04-24 14:40:04.894595: epoch 67 avg_loss = 0.616618\n",
      "2017-04-24 14:40:04.925822: epoch 68 avg_loss = 0.642123\n",
      "2017-04-24 14:40:04.958120: epoch 69 avg_loss = 0.659301\n",
      "2017-04-24 14:40:04.994212: epoch 70 avg_loss = 0.554205\n",
      "2017-04-24 14:40:05.027268: epoch 71 avg_loss = 0.603117\n",
      "2017-04-24 14:40:05.061562: epoch 72 avg_loss = 0.658819\n",
      "2017-04-24 14:40:05.095610: epoch 73 avg_loss = 0.604283\n",
      "2017-04-24 14:40:05.126683: epoch 74 avg_loss = 0.582791\n",
      "2017-04-24 14:40:05.159349: epoch 75 avg_loss = 0.611387\n",
      "2017-04-24 14:40:05.191731: epoch 76 avg_loss = 0.618939\n",
      "2017-04-24 14:40:05.231154: epoch 77 avg_loss = 0.522187\n",
      "2017-04-24 14:40:05.260855: epoch 78 avg_loss = 0.569136\n",
      "2017-04-24 14:40:05.294193: epoch 79 avg_loss = 0.622231\n",
      "2017-04-24 14:40:05.328950: epoch 80 avg_loss = 0.566482\n",
      "2017-04-24 14:40:05.361503: epoch 81 avg_loss = 0.548185\n",
      "2017-04-24 14:40:05.394005: epoch 82 avg_loss = 0.579596\n",
      "2017-04-24 14:40:05.426909: epoch 83 avg_loss = 0.576493\n",
      "2017-04-24 14:40:05.467179: epoch 84 avg_loss = 0.489199\n",
      "2017-04-24 14:40:05.498788: epoch 85 avg_loss = 0.534533\n",
      "2017-04-24 14:40:05.533338: epoch 86 avg_loss = 0.583964\n",
      "2017-04-24 14:40:05.566051: epoch 87 avg_loss = 0.526799\n",
      "2017-04-24 14:40:05.598199: epoch 88 avg_loss = 0.512199\n",
      "2017-04-24 14:40:05.630917: epoch 89 avg_loss = 0.546445\n",
      "2017-04-24 14:40:05.663963: epoch 90 avg_loss = 0.532010\n",
      "2017-04-24 14:40:05.703587: epoch 91 avg_loss = 0.455146\n",
      "2017-04-24 14:40:05.733723: epoch 92 avg_loss = 0.498936\n",
      "2017-04-24 14:40:05.766794: epoch 93 avg_loss = 0.543954\n",
      "2017-04-24 14:40:05.800710: epoch 94 avg_loss = 0.485762\n",
      "2017-04-24 14:40:05.832340: epoch 95 avg_loss = 0.474850\n",
      "2017-04-24 14:40:05.869468: epoch 96 avg_loss = 0.512133\n",
      "2017-04-24 14:40:05.902696: epoch 97 avg_loss = 0.486226\n",
      "2017-04-24 14:40:05.941740: epoch 98 avg_loss = 0.420456\n",
      "2017-04-24 14:40:05.974294: epoch 99 avg_loss = 0.462700\n",
      "2017-04-24 14:40:06.014391: epoch 100 avg_loss = 0.502917\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Sequence Labeling with variable-length sequences\n",
    "####\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "NUM_HIDDEN = 5\n",
    "NUM_CLASS = 3\n",
    "FEATURE_SIZE_PER_TIMESTEP = 5\n",
    "\n",
    "### Data pipeline\n",
    "def input_pipeline(filename, batch_size, epochs=None):\n",
    "    file_list = [os.path.join(os.getcwd(), filename)]\n",
    "    file_queue = tf.train.string_input_producer(file_list, num_epochs=epochs)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(file_queue)\n",
    "    sequence_features = {\n",
    "        \"inputs\": tf.FixedLenSequenceFeature([FEATURE_SIZE_PER_TIMESTEP], dtype=tf.float32),\n",
    "        \"label\": tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "    }\n",
    "    _, sequence = tf.parse_single_sequence_example(\n",
    "        serialized=serialized_example,\n",
    "        sequence_features=sequence_features)\n",
    "\n",
    "    actual_length = tf.shape(sequence[\"inputs\"])[0]\n",
    "    batch_lengths, batch_sequences, batch_labels = tf.train.batch(\n",
    "        [actual_length, sequence[\"inputs\"], sequence[\"label\"]],\n",
    "        batch_size=batch_size,\n",
    "        dynamic_pad=True,\n",
    "        allow_smaller_final_batch=True,\n",
    "        name=\"input_batching\")\n",
    "    return batch_lengths, batch_sequences, batch_labels\n",
    "\n",
    "\n",
    "def _last_relevant(outputs, actual_lengths):\n",
    "    \"\"\"\n",
    "    :param outputs: [batch_size x max_seq_length x hidden_size] tensor of dynamic_rnn outputs\n",
    "    :param actual_lengths: [batch_size] tensor of sequence actual lengths\n",
    "    :return: [batch_size x hidden_size] tensor of last outputs\n",
    "    \"\"\"\n",
    "    batch_size, max_seq_length, hidden_size = tf.unstack(tf.shape(outputs))\n",
    "    index = tf.range(0, batch_size) * max_seq_length + (actual_lengths - 1)\n",
    "    return tf.gather(tf.reshape(outputs, [-1, hidden_size]), index)\n",
    "\n",
    "\n",
    "### Build Model\n",
    "def inference(inputs, actual_lengths):\n",
    "    cell = tf.contrib.rnn.LSTMCell(NUM_HIDDEN)\n",
    "    outputs, current_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, sequence_length=actual_lengths)\n",
    "    last_outputs = _last_relevant(outputs, actual_lengths)\n",
    "    # Output layer weights & biases\n",
    "    weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_CLASS]), dtype=tf.float32)\n",
    "    biases = tf.Variable(tf.constant(0.1, shape=[NUM_CLASS]), dtype=tf.float32)\n",
    "    # Softmax classification based on outputs of the last time step of each sequence\n",
    "    logits = tf.add(tf.matmul(last_outputs, weights), biases)\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    return logits, predictions\n",
    "\n",
    "\n",
    "## Cost function\n",
    "def loss(logits, labels, actual_lengths):\n",
    "    labels_flat = tf.reshape(labels, [-1])\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_flat)\n",
    "    mean_loss = tf.reduce_mean(cross_entropy)\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "## Error tracking \n",
    "def error(predictions, labels, actual_lengths):\n",
    "    labels_flat = tf.reshape(labels, [-1])\n",
    "    errors = tf.not_equal(tf.argmax(predictions, 1))\n",
    "    mean_error = tf.reduce_mean(tf.cast(errors, tf.float32))\n",
    "    return mean_error\n",
    "    \n",
    "\n",
    "def training(loss, learning_rate):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)    \n",
    "    return train_op\n",
    "\n",
    "\n",
    "### Training\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 3\n",
    "DISPLAY_STEP = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAINING_SET_SIZE = 7\n",
    "\n",
    "filename = 'Sequence_classification.tfr'\n",
    "with tf.Graph().as_default():\n",
    "    tf.set_random_seed(10)\n",
    "    np.random.seed(10)    \n",
    "    # Build Graph\n",
    "    lengths, sequences, labels = input_pipeline(filename, BATCH_SIZE)\n",
    "    logits, _ = inference(sequences, lengths)\n",
    "    avg_loss = loss(logits, labels, lengths)\n",
    "    train_op = training(avg_loss, LEARNING_RATE)\n",
    "    \n",
    "    # Create & Initialize Session\n",
    "    sess = tf.Session()\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # Start QueueRunner\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    try: \n",
    "        # Training cycles\n",
    "        for epoch in range(1, NUM_EPOCHS+1):\n",
    "            epoch_avg_loss = 0.0\n",
    "            total_batch = int(TRAINING_SET_SIZE / BATCH_SIZE\n",
    "                ) + 1 if TRAINING_SET_SIZE % BATCH_SIZE != 0 else int(\n",
    "                TRAINING_SET_SIZE / BATCH_SIZE)\n",
    "            for step in range(1, total_batch +1):\n",
    "                if coord.should_stop():\n",
    "                    break\n",
    "                _, train_loss = sess.run([train_op, avg_loss]) \n",
    "                epoch_avg_loss += train_loss / total_batch\n",
    "                assert not np.isnan(train_loss), 'Model diverged with loss = NaN'\n",
    "                \n",
    "                if step % DISPLAY_STEP == 0:\n",
    "                    print('%s: epoch %d, step %d, train_loss = %.6f'\n",
    "                        % (datetime.now(), epoch, step, train_loss))\n",
    "                \n",
    "            print('%s: epoch %d avg_loss = %.6f'\n",
    "                % (datetime.now(), epoch, epoch_avg_loss))\n",
    "                \n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        print(e.error_code, e.message)\n",
    "        print('Done!')\n",
    "    \n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "    \n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "    \n",
    "print('Finished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
